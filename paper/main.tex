% 10+2 Due Fri 28 Aug anywhere-on-earth
%
% ICSE 2019 Technical Track submission must not exceed 10 pages,
% including all text, figures, tables, and appendices; two additional pages
% containing only references are permitted.
%
% ICSE 2019 Technical Track will employ a double-blind review process.
% Thus, no submission may reveal its authorsâ€™ identities.
%
% https://2019.icse-conferences.org/track/icse-2019-Technical-Papers#Call-for-Papers

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{myref}
\usepackage{url}
\usepackage{framed}
\usepackage{pgfplots}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\fixme}[1]{\textcolor{red}{FIXME: #1}}

\newcommand\lt[1]{{\lstinline|#1|}}
\lstset{language=python}
\definecolor{dkgreen}{rgb}{0,0.5,0}
\definecolor{dkred}{rgb}{0.5,0,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\lstset{basicstyle=\ttfamily\bfseries\footnotesize,
  morekeywords={virtualinvoke},
  keywordstyle=\color{blue},
  ndkeywordstyle=\color{red},
  commentstyle=\color{dkred},
  stringstyle=\color{dkgreen},
  numbers=left,
  numberstyle=\ttfamily\footnotesize\color{gray},
  stepnumber=1,
  numbersep=10pt,
  backgroundcolor=\color{white},
  tabsize=4,
  showspaces=false,
  showstringspaces=false,
  xleftmargin=.23in
}

\pagestyle{plain}
\pagenumbering{arabic}

\newsavebox{\verbsavebox}

\begin{document}

%
\title{Blaming the Typeless: \\ Scalable, Human-Centric Python Localization}

\iffalse

\author{\IEEEauthorblockN{Benjamin Cosman}
\IEEEauthorblockA{\textit{UC San Diego}\\
blcosman@eng.ecsd.edu}
\and
\IEEEauthorblockN{Leon Medvinsky}
\IEEEauthorblockA{\textit{UC San Diego}\\
lmedvinsky@eng.ecsd.edu}
\and
\IEEEauthorblockN{Ranjit Jhala}
\IEEEauthorblockA{\textit{UC San Diego}\\
jhala@cs.ecsd.edu}
\and
\IEEEauthorblockN{Westley Weimer}
\IEEEauthorblockA{\textit{University of Michigan}\\
weimerw@umich.edu}
}

\fi

\author{\IEEEauthorblockN{\emph{submitted for double-blind review}}}

\maketitle

\begin{abstract}
Abstract
\end{abstract}

\section{Introduction}

Dynamically-typed languages are becoming increasingly common for rapid
application development, full-scale software engineering, and
pedagogy (e.g.,~\cite{FIXME,FIXME}). Languages like Python, Ruby and Lua
often feature a gentle learning curve and attractive facilities, from
familiar garbage collection and regular expressions to more expressive and
exotic list comprehensions and higher-order function blocks. One side
effect of popularity with novices and students, however, is that many
programmers of such languages may lack expertise when presented with
dynamic type errors, and may thus struggle to interpret and localize
them~\cite{orso-parnin,Zhang2014-lv,Christiansen2014-qc,Pavlinovic2014-mr,Chen2014-gd,Neubauer2003-xv,Stuckey2004-ne}.
We propose a machine learning approach based on static, dynamic and
contextual information that effectively localizes beginner type errors.

The relative costs and benefits of static vs. dynamic error detection are
well-studied. Despite this, however, many currently-available techniques
are ill-suited to address the problem of localization for type errors in
dynamic languages. First, relying on the extant language interpreter
requires test inputs
(cf.~\cite{Godefroid2005-am,Naylor2007-mi,Cadar2008-kg,PAcheco2007-at}),
may involve error messages that are difficult for novices to
understand~\cite{Marceau2011-ok,Marceau2011-cy,Nienaltowski2008-bv,Jadud2006-ly,Ishii2014-nf},
and may misleadingly implicate the symptom rather than the
cause~\cite{zeller05,Jose2011}.  Second, a
significant amount of development is carried out via IDE or web interfaces,
which require tools that operate on code fragments~\cite{Guo2013-vu}.
Third, attempts to retrofit static type systems into dynamic languages,
such as TypeScript~\cite{typescript2014,refscriptpldi16,mollerOOPSLA14} for
JavaScript and various
gradual and static type approaches for 
Python~\cite{mypy,gradual-typing-python} or
Ruby~\cite{static-typing-ruby,an11,foster09,foster11}
are promising but not always widely
deployed. Finally, research tools and algorithms for fault
localization either require rich static types (e.g., as in
Mycroft~\cite{FIXME}, Nate~\cite{learning-to-blame} or Sherrloc~\cite{FIXME}), require
test cases (e.g., as in Tarantula~\cite{tarantula}), or produce voluminous
ranked output lists that have been found to be unhelpful in general and
less useful to novices in particular~\cite[Sec.~5.1]{orso-parnin}.

We focus on Python as an indicative, popular dynamically-typed language.
We desire a fault localization algorithm for Python type errors that
will agree with human intuition (\emph{accuracy}), will apply to
off-the-shelf, unannotated beginner-written program fragments
(\emph{generality}), and will scalably apply to industrial-strength
deployments (\emph{scalability}). Our key insights are (1) that we can leverage
modern supervised machine learning for accuracy and an agreement with human
norms; (2) that we can use static, dynamic, contextual and slice-based
features to generalize to off-the-shelf beginner programs; and (3) that we
can use large beginner datasets, such as those from
PythonTutor.com~\cite{Guo2013-vu}, to assess scalability.

We propose an algorithm that takes as input a Python program fragment that
produces an uncaught runtime exception such as a type error. Via machine
learning over a carefully-selected set of program-relevant features, we
produce the expression most likely implicated in the exception according to
our learned model. We evaluate on an entire year of buggy Python 3 program
executions on PythonTutor.com --- over 270,000 instances --- measuring
accuracy with respect to the human actions as the ground truth and
comparing against the vanilla Python interpreter as a baseline.

The contributions of this paper are as follows:
\begin{enumerate}

\item We present an algorithm for accurately localizing dynamic type errors
in Python. We find that our approach is 60--79\% accurate, compared to
the interpreter baseline of 45\%.  

\item We describe and evaluate static, dynamic, contextual and slice-based
features for generalizing machine-learning fault localization to
beginner-written Python fragments. We find such features are all critical
to our model, but that our model is not sensitive to error type and
performs similarly across defect classes.  

\item We analyze the results of a thorough evaluation on over 270,000 real
user instances, show that it is a more heterogeneous dataset than those used
in previous work, and demonstrate the scalability of our algorithm.

\end{enumerate}

\section{Motivating Example}
\label{sec-motex}

\begin{figure}
\begin{lstlisting}
year = int(time.strftime("%Y"))
age = input("Enter your age")
print("You will be twice as old in:")
print(year + age)
\end{lstlisting}
\caption{
\label{fig-motex}
A program with multiple fault localizations (lines 1 and 2).}
\end{figure}

One of the key difficulties in localizing type errors, especially for
dynamically-typed programs, is that there are often multiple
logically-valid ways to address a type error~\cite{FIXME} --- only one of
which may be desired by the programmer~\cite{FIXME}.  Consider the program
in \figref{motex}, adapted from our dataset of human submissions. The
program attempts to carry out some simple arithmetic based on a given
number and the current year. When executed, however, the program raises an
exception on line 4, related to the addition of an integer variable to a
string variable.

One reasonable fix (and the one that the programmer actually used in this
case) is to add an \lt{int()} cast on line 2. However, another possible
``fix'' would be to \textit{remove} the \lt{int()} cast from line 1, in
which case both variables hold string values at runtime, and the addition
on line 4 would be interpreted as string concatenation. While the second
fix creates a well-typed program, it is less likely to correspond to
developer intent, and is thus less likely to be helpful as a debugging aid.

This simple example highlights a key fault localization challenge in such
dynamically-typed settings: a runtime type error alone may not contain
enough information to pinpoint the human-preferred localization from among
many valid-but-less-helpful localizations. Our insight is that the relevant
information is not in the types (e.g., string or integer) alone but in the
context in which the values are used.  We propose an approach based on
machine learning over a combination of static and dynamic contextual
features.

% TODO: would also be nice if the example showed that the place Python crashes
% may not be a place that should be fixed?

\section{Algorithm Overview}
\label{sec-algorithm} 

We present an algorithm for accurately localizing faults~\cite{tarantula} in
dynamically-typed, beginner-written Python programs that exhibit
non-trivial uncaught runtime exceptions. We do not consider syntax errors
or references to undefined variables. Our algorithm uses machine learning
models based on static, dynamic, contextual and slicing features to
implicate suspicious expressions. Since studies have found that voluminous
fault localization output is not useful to
developers~\cite{orso-parnin,orso-parnin2015}, we focus on producing Top-1
and Top-3 rankings.

Our algorithm first extracts static and dynamic features from a Python
program (Section~\ref{sec-features}). Next, using a labeled training
corpus, we learn a machine learning model over those features
(Section~\ref{sec-model}). Once the model has been learned, we localize
faults in new Python programs by extracting their features and applying the
model.

Drawing inspiration from localization algorithms such as
Nate~\cite{learning-to-blame} and the natural language processing term
frequency vector (or ``bag of words'') model~\cite{FIXME}, we represent
each buggy program as a ``bag of abstracted terms''. A \emph{term} is
either a statement or expression.

\subsection{Model Feature Intuition and Extraction}

We observe that many type errors admit multiple logically valid resolutions
(see \secref{motex}): we thus cannot expect to match programmer desires
through type constraints alone, and instead propose to leverage information
from multiple modalities. We use static features to capture structured
program meaning, contextual features to capture the relationship between a
program fragment and its environment, and dynamic features (including slice
information) to reason about conditional behavior.

For each term in the program we compute static, dynamic, and contextual
features. While some features can be computed directly from syntactic
information, others are only accessible dynamically. In a
dynamically-typed language such as Python, this includes types of each
expression. Since programs are highly structured and expressions gain
meaning in relation to their surroundings, we also include contextual
features.

While static features can be computed via parsing, dynamic features require
executing the program fragment. To calculate dynamic features, we first
convert each buggy program to an equivalent program in a form similar to
A-normal form \cite{anf}: all intermediate expressions are split out and
assigned to new temporary variables.
We then run the resulting program through the PythonTutor
backend~\cite{Guo2013-vu}.  This backend is based on the Python debugging
library BDB~\cite{FIXME}; it returns a trace of the program along with the
state of the heap at each execution step. That trace is used to compute
dynamic features.

\subsection{Model Features}
\label{sec-features}

\subsubsection{Syntactic Forms (Static)}

We hypothesize that certain syntactic categories of terms may be more
prone to bugs than others, especially for beginner programmers.  For
example, students might have more trouble with loop conditions than with
simple assignments. The first feature we consider is the syntactic
category of node. This feature is categorical, using standard abstract
syntax tree categories such as Return or Import for statements and Variable
or Application for expressions.

\subsubsection{Expression Size (Static)}

This numeric feature counts the number of descendent nodes in the subtree
rooted at that node. Our intuition is that larger, more complex expressions
may be more prone to faults.

\subsubsection{Type (Dynamic)}

Although type constraints alone cannot always match human intuition for
fault localization problems, types still contain very useful information.
We observe that some types may be inherently suspect whenever they appear,
especially for beginner-written code. For example, there are very few
reasons to have a variable of type \lt{NoneType} in Python. We model
type information as a categorical feature. Possible
values include all the basic Python types (such as \lt{int}
and \lt{tuple}) as well as three special values:
\begin{itemize}
    \item \lt{Statement} --- Given to all statements, since our terms
    include both statements and expressions but only expressions
    have types
    \item \lt{Unknown} --- Given to expressions that are never evaluated in the dynamic
    trace
    \item \lt{Multiple} --- Given to expressions which are evaluated multiple times in
    a trace and do not always have the same type
\end{itemize}

\subsubsection{Slice (Dynamic)}
The goal of this feature, roughly equivalent in purpose to the type error
slice in Nate~\cite{learning-to-blame}, is to help eliminate terms that
cannot be the source of the crash. We compute a dynamic program
slice~\cite{KOREL1988155}: a set of terms that contributed at runtime to
the observed exception. This boolean feature encodes whether the term
is a member of the slice. Our slicing algorithm, which balances scalable
simplicity with coverage for beginner-written programs, is presented in
\secref{slice-algorithm}.

\subsubsection{Crash Location (Dynamic)} We observe that the precise term that
causes Python to crash is, in fact, the one the user decides to change
much of the time. While this information is not sufficient on its own
(cf. the vanilla Python interpreter, which largely uses this feature),
it can be a very helpful supplement. This boolean feature encodes whether
or not the term is exactly the source location of the uncaught exception.

\subsubsection{Exception Type (Dynamic)} The type of error thrown by the program
contains useful information for its localization. In particular, a term may be
more or less suspicious depending on the ultimate uncaught exception. As an
example, every division term may be more suspicious if the uncaught exception
being considered is \lt{DivisionByZero}. We encode the exception type as
a categorical feature.
% to \emph{every} vector derived from that program.

\subsubsection{Contextual features}

Term frequency vectors in general, and our bag of abstracted terms
representation by extension, do not typically include contextual
information such as ordering or relative positioning. As with many other
analyses, such as contextual operational semantics~\cite{FIXME}, we observe
that the meaning of an expression may depend on its surrounding context.

As an example, consider the \lt{0} terms in both \lt{x = 0} and \lt{x / 0}.
Both instances of \lt{0} have the same syntactic form, size and type
(etc.). We may prefer to implicate the \lt{0} in \lt{x / 0} as suspicious,
especially for beginner-written programs, but cannot distinguish it without
surrounding contextual information. Data structures such as abstract syntax
trees and control flow graphics capture such contextual information, but
are not immediately applicable to machine learning.

We desire to encode such information while retaining the use of scalable,
accurate off-the-shelf machine learning algorithms that operate on feature
vectors. We thus propose to embed contextual information in a vector,
borrowing insights from standard approaches in machine learning. We
associate with each term additional features that correspond to the features of
its parent and child nodes. For representational regularity, we always
model three children; terms without parents or children are given special
values (e.g., zero or NotApplicable) for those contextual features.

\subsection{Dynamic Slicing Algorithm}
\label{sec-slice-algorithm}

\begin{figure}
\begin{lrbox}{\verbsavebox}
\begin{lstlisting}[xrightmargin=0.5\linewidth]
x = input() # 42
if x < 20:
  y = 5
  z = 123
else:
  y = 0
  z = 321
assert(y != 0)
\end{lstlisting}
\end{lrbox}
~ \hfill
\subcaptionbox{Pre-Slicing}{\usebox{\verbsavebox}}
\hfill
\begin{lrbox}{\verbsavebox}
\begin{lstlisting}[xrightmargin=0.5\linewidth]
x = 42
if x < 20:


else:
  y = 0

assert(y != 0)
\end{lstlisting}
\end{lrbox}
\subcaptionbox{Post-Slicing}{\usebox{\verbsavebox}}
\hfill
~
\caption{
  A program being sliced (left) and the resulting slice (right) with
  respect to the assertion on line 8.
}
\label{fig-slice-example}
\end{figure}

\begin{figure}
\begin{lstlisting}
x = input() # The user inputs 0
if x != 0:
  x = 1
print("One over your number is: %d" % (1 / x))
\end{lstlisting}
\caption{In this program, the programmer intended the placeholder
  assignment \lt{x = 1} to be used in case the user inputs 0.
  The defect is that the
  incorrect condition \lt{x != 0} (rather than \lt{x == 0}) was employed. However, the
  condition expression will not be included in the slice (which would include
  lines 1 and 4), because the presence of
  the conditional does not affect the behavior of the program.
}
\label{fig-slice-downside-example}
\end{figure}

\begin{figure}
\begin{lstlisting}
while true:
  x = input()
  if x != 0:
    break
  print("One over your number is: %d" % (1 / x))
\end{lstlisting}
\caption{
  In this example, the programmer attempts to handle invalid input by
  breaking out of the input loop. Our method
  records a control dependency between the condition on line 3 and the
  print statement on line 5, correctly placing the buggy condition in the
  program slice.
}
\label{fig-early-break}
\end{figure}

Slicing information can prevent our model from implicating irrelevant
nodes in the fault localization. Program slicing is a well-studied
field with many explored tradeoffs (e.g., see Xu et al. for a
survey~\cite{xu2005}). We desire a slicing algorithm that can be computed
efficiently (to scale to hundreds of thousands of instances) but that will
also admit high accuracy: we achieve this by focusing on features relevant
to beginner-written programs. We
follow the basic approach of Korel and Laski~\cite{KOREL1988155,
KOREL1990187}, building a
graph of data and control dependencies. We then traverse the graph
backwards, starting at the execution step where the error occurred, to
collect the set of terms that the excepting line transitively depended on.
This excludes lines that could not have caused the exception, such as lines
that never ran, or lines that ran but had no connection to the step where
the exception happened.

\figref{slice-example} is an example of the input and output of the slicing
algorithm. The slicer takes as input a python source string and a list of inputs,
to be fed into the program whenever \lt{input()} is invoked. The slicer outputs
the program elements included in the slice.

The slicer proceeds by creating an execution trace of the example program.
At each execution step the trace includes the line number being run and the state of the
heap and variables. In the example in \figref{slice-example},
lines $1, 2, 3, 4$ and $8$ are executed.
It then iterates forward through the trace, recording which
execution steps have a define-use relation, and which execution steps
have a test-control relation. The define-use relation is between a step where
the value of a variable is used, and the step where that variable was last
defined. In \figref{slice-example}, a define-use relationship exists
between lines 1 and 2 and between 3 and 8. A test-control relation is between
the execution of a control flow statement and any statements that were run as
a result of it. In \figref{slice-example}, a test-control relationship exists
between the if statement at line 2 and the assignments on lines 3 and 4. After
building up these relations, the slicer starts at the execution step causing an
exception (the execution of line 8), and builds the exception's set of
dependencies by iteratively adding to the set any step that has a define-use
or test-control relationship with a step already in the set. Finally, the
algorithm maps all of the execution steps to their corresponding program
element locations and returns the result.

To balance ease of prototype implementation against coverage for
beginner-written programs, our slicing algorithm handles every syntax node
supported by the Python3 standard \lt{ast} library except:
\begin{itemize}
\item Assignments where the left hand side is not a variable or a simple chain
  of attribute indexing or subscripting
\item Assignments where attribute indexing or subscripting on the left hand side
  means something other than the default, (e.g., if the operations are
  overridden by a class)
\item Lambda, generator and starred expressions
\item Set and dictionary comprehensions
\item Await, yield, and yield from
\item Variable argument ellipsis
\item Coroutine definitions and asynchronous loops
\item Delete, with and raise statements
\end{itemize}

A downside of using a dynamic (as opposed to static) slice, in the case of
control dependencies, is that it excludes terms whose presence does not
affect whether the exception happened, but which could have had an effect
if the term was different. In
\figref{slice-downside-example}, the conditional statement is never
executed, so the removal of the conditional would not have any effect on
the program's behavior, and it would not be included in a more precise
program slice. In this example, the dynamic slice misses the incorrect
test condition because the very defect we are trying to localize
caused the conditional statement and its body to become irrelevant.

To overcome these issues in the ``early return'' or ``early break'' case
(\figref{early-break}), we check if a break, return, or other statement for
escaping structured control flow is present inside of a conditional
statement. We then add dependencies in the execution trace's dependency graph
between the enclosing conditional and the statements that would have been skipped
by the break or return. While this heuristic is effective in practice, it
does not overcome all related problems: we thus treat dynamic slice
information as one of many features rather than as a hard constraint.
This boolean feature tracks whether or not the term is part of the final
dynamic slice.

\subsection{Machine Learning Model Generation}
\label{sec-model}

\fixme{
\emph{Models} The following descriptions of Decision Trees and
Random Forests will have to be redone as they are copied from Learning to Blame.
}

\fixme{how the data is represented}

\fixme{is program pair the correct term?}

To apply machine learning to analyze the data, we have to formulate the
problem into standard classification problem.
For each line of a program pair, we extracted the features mentioned in
Section \ref{sec-features}.
By performing one-hot encoding on categorical features, we can represent the
extracted features into a length $M$ feature vector $\mathbf{x} \in
\mathcal{R}^{M}$.
For the corresponding label, is represented as $1$ and $0$, which means whether
the line blamed or not.
Let the number of examples be $N$.
Given a data set $\mathcal{D} \in \{\mathbf{x}^{(i)}, y^{(i)}\}^{N}_{i=1}$,
where $\mathbf{x}^{(i)} \in \mathcal{R}^{M}$ and $y^{(i)} \in \{0, 1\}$,
a learning model will learn a function $f: \mathbf{R}^{M} \to \{0, 1\}$.
Due to the large amount of data provided in this task, we only consider the
following models which are scalable to large data set.

\emph{Decision Tree}
Decision tree~(DT) learns a tree of binary predicates over the features and
recursively partitioning the feature space until a final classification can
be made.
%
Each node in the tree contains a single thresholding $\mathbf{x}_i \leq t$, where
applying threshold $t \in \mathcal{R}$ on the $i$-th feature of $\mathbf{x}$,
which determines whether a given input should proceed down the left or right
subtree.
%
Each leaf is labeled with a prediction and the fraction of
correctly-labeled training samples that would reach it; the latter
quantity can be interpreted as the decision tree's confidence in its
prediction.
%
This leads to a non-linear prediction rule which can be more expressive
then linear model like LR depending on the data used to build it.
%Training a decision tree entails finding both a set of good partitioning
%predicates and a good ordering of the predicates based on data.
%
The training of a decision tree is usually done in a top-down greedy manner,
and there are several implementations of DT such as C4.5
\cite{quinlan2014c4} and CART \cite{breiman2017classification}.

%Another advantage of decision trees is their ease of interpretation ---
%the decision rule is a white-box model that can be readily described to
%a human, especially when the tree is small.
%%
%However, the main limitation is that these trees often do not generalize
%well, though this can be somewhat mitigated by \emph{pruning} the tree.

\emph{Random Forest}
%
Random forest (RF)~\cite{breiman2001random} improves generalization by
learning an \emph{ensemble} of decision trees.
For prediction, each tree in the forest votes on one of the classes.
The percentage of vote for each class forms a natural confidence score.
During training, RF selects a subset of training data by sampling with
replacement and use the subset to train each of the tree.
By doing so, RF is able to generate a forest with each tree trained on
a different subset of data.
The diversity of the underlying models tends to make RF less susceptible to
the overfitting, but it also makes the learned model more difficult to
interpret.
%
%Since each classifier in the ensemble is a decision tree, this still
%allows for complex and expressive classifiers.

%The training process involves taking $N$ random subsets of the training
%data and training a separate decision tree on each subset --- the
%training process for the decision trees is often modified slightly to
%reduce correlation between trees, by forcing each tree to pick features
%from a random subset of all features at each node.
%

\emph{Training methodology}
At this point we have feature vectors describing every term of every program in
our data set. The vectors from a random 30\% of the program pairs are set aside for testing and
the other 70\% are used for training.
Within the training data, we use 3-fold
cross-validation to select the best parameters for our models, before training
using those parameters on the full training set and then scoring the models
using the testing set.
Each trained model takes in any vector representing a term
in a buggy program, and returns a confidence score representing how likely it is that
that term was one of the terms changed between the fixed and buggy versions.
Thus we can treat the model as providing a \emph{ranking} over all terms by
confidence, where the top result is the one that the model deems most likely
to be changed.
For a given $k$, we score the model based on Top-$k$ accuracy: for what
proportion of the programs is a correct answer (i.e. a term that is actually
changed) present in the top $k$ results.
We retrain the models for each value of k, since a model optimized to produce
the single best result might be different from one optimized to get a good
result into the top three.

For decision tree and random forest, we use the implementation from
\textsc{scikit-learn}~\cite{scikit-learn}.
% Due to the large amount of data points, we did not use \textsc{scikit-learn}'s
% implementation for logistic regression and multilayer perceptron.
% We implementation both model using
% \textsc{keras}~\cite{chollet2015keras} with mini-batch gradient descent.

Unsurprisingly this is an imbalanced dataset in that negative examples, i.e.
terms that are not changed by the programmer, are much more common than
positive examples, so during training we re-weight each example to the reciprocal
of the frequency of corresponding class.

\emph{Hyper-parameters}
All hyper-parameters are tuned using three-fold cross-validation on the
intended score (Top-$k$ accuracy).
For RF, the maximum depth of each tree is searched from $25$ to $35$ and the
number of tree are fixed to $500$.
For decision tree maximum depth of the tree is searched from $25$ to $35$ with interval of $5$,
the minimum impurity decrease for each split is searched from $10^{-7}$ to
$10^{-3}$ with multiple of $10^{-2}$ and the minimum samples in each leaf is set to $200$.
For logistic regression and MLP, binary cross-entropy is set as the loss
function and Adam \cite{kingma2014adam} optimizer is used.
Learning rate is set to $0.1$ and the number of epochs are $10$.
L2 regularization is applied to the weights of both models,
the weight for L2 term is searched from $10^0$ to $10^{-5}$ with a multiple of $10^{-1}$.
Other parameters that are not mentioned are kept with the default setting of the
original packages.

%In each case the most effective model we trained was a decision tree. (The other
%models we tried were Logistic Regression and MLP. Yao-Yuan - anything to add to
%this section? in particular, is it actually accurate to say we're doing
%cross-validation - the 3-fold thing we do is actually just to select parameters,
%and then there's no cross-validation on the final model, right?
%%
%Correct
%)


\section{Evaluation}
\label{sec-eval}

We conducted a large-scale empirical evaluation of our algorithm with the
aim of addressing a number of research questions:
\begin{enumerate}

\item[RQ1]{Can we accurately localize non-trivial faults in Python
programs?}

\item[RQ2]{Which features are the most important for Python fault
localization overall?}

% \item[RQ3]{Which features are the most important for various categories of
% Python defects?}

\item[RQ3]{How well does our algorithm handle different kinds of Python errors?}

\item[RQ4]{Is our algorithm accurate on heterogeneous sets of programs?}

\end{enumerate}

\subsection{Dataset and Program Collection}

Our raw data consist of every Python 3 program that a user executed on
PythonTutor.com~\cite{Guo2013-vu} (not in ``live'' mode) during 2017, other
than those with syntax errors or undefined variables.  Each program which
crashes (throws an uncaught Python exception) is paired with the next
program (by the same user) that does not crash, under the assumption that
the latter is the fixed version of the former. We discard pairs where the
difference between crashing and fixed versions is too high (more than a
stddev above average), since these are most likely to be violations of that
assumption (i.e., the program that does not crash is unrelated to the
crashing program). We also discard submissions that are out of scope given
PythonTutor's policies (e.g., very long-running executions or those that 
use forbidden libraries).

Ultimately, the dataset used in this evaluation contained 
272,534 usable pairs of programs.

\subsection{Labeled Training and Ground Truth}
\label{sec-training} 

Our algorithm is based on supervised machine learning and thus requires
labeled training instances.  For these experiments, we employ a random
forest our machine learning algorithm, finding it to provide the best
accuracy on our dataset with a scalable training and testing time.  In
addition, our evaluations require a ground truth notion of which terms
correspond to correct fault localizations.

We use the terms changed involved in the fixes by actual users as our
ground truth and training information.  Many PythonTutor users use the site
in an iterative fashion: they start out by writing a program that crashes,
and then edit it until it no longer crashes. Our dataset contains only
those crashing programs for which the same user later submitted a program
that did not crash. We compute a tree-diff~\cite{tree-diff} between the
originaly, buggy submission and the first fixed submission. We define
the \emph{ground truth} correct answer to be the set of terms in the
crashing program that also appear in the diff. We discuss the implications
of this choice in \secref{threats}.

Given that notion of ground truth, a candidate fault localization answer is
\emph{accurate} if it is contained in the ground truth set. That is, if the
human user changed lines $X$ and $Y$, a technique (either our algorithm 
or a baseline) is given credit for returning either $X$ or $Y$. When
considering ranked lists, a response list is \emph{top-$k$ accurate} if any one
of the top $k$ answers is contained in the ground truth set. \fixme{Read
this paragraph and correct it for truth; it is currently simply WRW's best
guess and I don't actually know what you did.} 

In our evaluation, we used FIXME as a training set and FIXME as a held-out
evaluation set. We employed cross-validation~\cite{kohavi} to help address
the potential threat of overfitting.

\subsection{RQ 1 --- Fault Localization Accuracy}

\begin{figure}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    symbolic x coords={Baseline,Top-1,Top-2,Top-3},
    xtick=data,
    ymin=0,
    ylabel=Model Accuracy,
    enlarge x limits=0.3,
    legend style={at={(0.5,-0.15)},anchor=north}
]
    \addplot table[x=scoreName, y=score, col sep=comma]{fault-localization-random-forest.csv};
\end{axis}
\end{tikzpicture}
\caption{Fault localization accuracy.
Baseline is the normal Python interpreter. The Top-$k$ bars represent
our approach, using random forests,
on the largest dataset (272534 pairs). The strong performance of our
algorithm on a large, real-world dataset is the primary result of this paper. 
}
\label{fig-full-dataset-acc-random-forest}
\end{figure}

We first use the full dataset to train and test decision forests optimized for
Top-1, Top-2, and Top-3 accuracy. As shown in \figref{full-dataset-acc-random-forest},
these produce a correct answer 60\%, 72\%, and
79\% of the time (to two significant figures). As a baseline, we determine that
the expression blamed by the standard Python interpreter (i.e., the one that
raises the uncaught exception) is only changed by the user 45\% of the
time. 

Thus, our most directly comparable model (Top-1), significantly outperforms
this baseline. Users who are only willing to look at a single error message
would have been better-served by our Top-1 model on this historical data. 

In addition, previous studies have shown that developers are willing to use
very short ranked lists~\cite[Sec.~5.6]{orso-parnin}, but not voluminous
ones. Our Top-3 model accuracy of 79\% dramatically improves upon the
current state of practice for scalable type-error localization in Python. 

% Goal: Establish that our algorithm works. This is the most basic question,
% but also the most relevant. If the reader takes away nothing else, they
% should conclude that our algorithm is effective.

\subsection{RQ 2 --- Feature Predictive Power}

\begin{table}[]
\begin{tabular}{llc}
Name                      & Category                & Gini Importance \\ \bottomrule
Parent size                    & Contextual (Syntactic)  & 0.112 \\
Child3 is statement           & Contextual (Syntactic)  & 0.061 \\
Parent is list literal         & Contextual (Syntactic)  & 0.055 \\
Program crashes here      & Dynamic (Error location)& 0.039 \\
Type is unknown           & Dynamic (Type)          & 0.037 \\
Parent type is unknown         & Contextual (Type)       & 0.037 \\
Err message is IndexError & Dynamic (Error message) & 0.033 \\
Size                      & Syntactic               & 0.028 \\
Parent is dictionary & Contextual (Syntactic)  & 0.027 \\
Child1 size                   & Contextual (Syntactic)  & 0.024 \\
Is variable               & Syntactic               & 0.020 \\
Child1 type is unknown        & Contextual (Type)       & 0.020 \\
\toprule
\end{tabular}
\caption{Feature predictive power (for a Top-3 Decision Tree
learned on the entire dataset). Parent and Child1--3 refer to the parent and
first three children of the node in question.}
\label{tab-feature-predictive-power}
\end{table}

\begin{table}[]
\begin{center}
\begin{tabular}{llc}
Name                     & Category                & F-score \\ \bottomrule
Child3 is statement          & Contextual (Syntactic)  & 69 \\
Size                     & Syntactic               & 63 \\
Program crashes here     & Dynamic (Error location)& 50 \\
Parent type is list           & Contextual (Type)       & 36 \\
Child1 type is bool          & Contextual (Type)       & 30 \\
Parent type is unknown        & Contextual (Type)       & 28 \\
Child1 is expression         & Contextual (Type)       & 25 \\
Child3 size                  & Contextual (Syntactic)  & 20 \\
Child1 is binary or unary op & Contextual (Syntactic)  & 20 \\
Child1 size                  & Contextual (Syntactic)  & 18 \\
Parent type is dictionary     & Contextual (Type)       & 17 \\
Type is string           & Dynamic (Type)          & 12 \\
\toprule
\end{tabular}
\end{center}
\caption{Feature predictive power as measured by ANOVA.  The F-score in
shown in
thousands to two significant figures; all p-values are less than 0.01.
Parent and Child1--3 refer to the parent and first three children of
the node in question.}
\label{tab-anova}
\end{table}

\begin{figure}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    symbolic x coords={No Context, No Types},
    xtick=data,
    ymin=0,
    ymax=1.1,
    ylabel=Normalized Model Accuracy,
    enlarge x limits=0.5,
    legend style={at={(0.5,-0.15)},anchor=north}
]
    \addplot table[x=scoreName, y=score, col sep=comma]{removing-features-2-normalized.csv};
    \draw [red] ({rel axis cs:0,0}|-{axis cs:No Types,1}) -- ({rel axis cs:1,0}|-{axis cs:No Types,1}) node [pos=0.33, above] {};
\end{axis}
\end{tikzpicture}
\caption{Normalized accuracy when categories of features are removed, based
on Top-1 decision trees on a random subset of 20,000 program pairs, as
compared to a Top-1 decision tree trained on all features.
\fixme{will add No Syntax bar}
}
\label{fig-removing-features}
\end{figure}

Having established the efficacy of our approach, we now investigate which
elements of our algorithmic design (\secref{algorithm}) contributed to that
success. 
\tabref{feature-predictive-power} summarizes the relative importance
of the top features in our model. The features are ranked by their
Gini importance (or mean decrease in impurity), a common measure
for decision tree and random forest models~\cite{breiman2001random}. Informally, the
Gini importance conveys a weighted count of the number of times a feature
is used to split a node: a feature that is learned to guide more model
classification decisions is more important. Similarly, \tabref{anova} gives
an alternate view of the relative feature importances, as measured by a
standard analysis of variance (ANOVA).

Some features, such as ``program crashes here'' and features
related to term size, are fairly direct. Others, such as ``Child3 is
statement'', a contextual, syntactic feature that was found to be very
important, merit additional explanation. In practice, we found it to
encode FIXME, since Python terms with statements in the third child node
position are FIXME. Similarly, features that determine if the parent node
is a list or dictionary are relevant because of FIXME. In beginner-written
Python we find that defects and typos often in the immediate child position
of a list or dictionary term because of FIXME. Finally, we note the
importance of our \lt{Unknown} type feature for expressions that are never
evaluated in the given trace: we find that they are relevant because of
FIXME. \fixme{Ben (etc.) must actually fill in real explanations here.} 

More broadly, we also present the results of a leave-one-out analysis in which
entire categories of features are removed and the model is trained
and tested only on those that remain. \figref{removing-features} shows
that there is a significant decrease in accuracy in each case. We note,
however, that leave-one-out analyses underestimate importance in cases of
feature overlap. For example, if a small program contains a string bug but
only a few string variables, both type information and slice information
are likely to implicate similar terms.

In all cases we see that syntactic, dynamic, and contextual features (i.e.,
the design decisions of our algorithm) are crucial to our algorithm's
accuracy. 

% prereq: define predictive power (leave-one-out, leave-one-in, Relief-F,
% ANOVA, whatever)

% Use as a rough guide: Table 3 on Page 8 of
% https://web.eecs.umich.edu/~weimerw/p/weimer-icsm2010.pdf

% Goal: Establish that we are smart for including all of these features. Note
% which individual features or feature categories were (not) included in
% previous work.

% Give a simple narrative about our effectiveness: implicitly, we were smart
% for deciding to include these features, better ingredients make better
% results, we have great results.

% TODO: what about turning on/off slicing?

% TODO: what about turning on/off types? (is that separate from slicing?)

\subsection{RQ 3 --- Defect Categories}

\begin{figure}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    symbolic x coords={Index, Value, Key, Type, Attribute},
    xtick=data,
    ymin=0,
    ylabel=Normalized Model Accuracy,
    legend style={at={(0.5,-0.15)},anchor=north}
]
    \addplot table[x=scoreName, y=score, col sep=comma]{defect-categories-2-normalized.csv};
    \draw [red] ({rel axis cs:0,0}|-{axis cs:Index,1}) -- ({rel axis cs:1,0}|-{axis cs:Index,1}) node [pos=0.33, above] {};
\end{axis}
\end{tikzpicture}
\caption{Accuracy when a Top-1 decision tree is trained and tested on only the data exhibiting
the error on the x-axis, as compared to a Top-1 decision tree on all the data.
}
\label{fig-defect-categories}
\end{figure}

We investigate the sensitivity of our algorithm to different categories of
Python errors: is our accuracy sensitive to certain kinds of user defects? 
We investigate training and testing Top-1 decision trees on only those
subsets of the dataset corresponding to each of the five most common
uncaught exceptions: \lt{TypeError}, \lt{IndexError}, \lt{AttributeError},
\lt{ValueError}, and \lt{KeyError}. Together, these five exceptions make
up FIXME\% of the errors in our dataset. 

As shown in \figref{defect-categories}, these per-defect models have
normalized accuracy between 86\% and 115\% of a comparable model trained on
the dataset as a whole. This shows that our algorithm is robust and able to
give high-accuracy fault localizations on a wide variety of defect types. 
Having consistent, rather than defect-type-sensitive, performance is
important for debugging-tool
usability~\cite{orso-parnin,Bessey2010,ayewah10}. 


% prereq: define defect category. This could be either the raw python
% exception name or it could be clusters that we have manually created.
% Ranjit notes: result could be ``we need everything for everything'', at
% which point this is useless and should be skipped. Per Ben's exam: may find
% that for different categories of errors we should use different ML
% classifiers.

% PLOT Type: Bar Graph (three bars per point on the X axis)

% PLOT Y Axis: Leave-One-In Accuracy (normalized per X axis point)
% Ranjit notes: on smaller subsets we could rebuild the classifier each time.
% Wes notes that if we do, it takes us longer. If we re-use the monolithic
% model we can do this very quickly.

% PLOT X AXis: Defect Categories

% PLOT Bars: Static, Dynamic, Context

% Goal: Demonstrate that each category of feature is essential for a certain
% piece of the problem. We need ABC feature to handle DEF class of defects,
% but ABC feature does not work well on GHI class of defects, for those we
% need JKL features. Implicit: we were smart for including all of these
% categories.

\subsection{RQ 4 --- Diversity of Programs}

% PLOT LINES: line 1 is ``Nate programs'', line 2 is ``25\%, at random, of our dataset''

% \begin{figure}
% \foreach \method in {single, complete, average}
% {
% \begin{tikzpicture}
%   \begin{axis}[
%       xlabel=Threshold,
%       ylabel=\# of clusters,
%     %   ytick distance=2,
%       title=Linkage: \method
%     ]
%     \addplot+[mark=none] table[x=threshold, y=python_twenty_cluster_counts_\method, col sep=comma]{cluster_counts.csv};
% \end{axis}
% \end{tikzpicture}
% }
% \caption{The number of clusters when clustering a baseline of twenty dissimilar programs from
%   Rosetta Code. We give no plot for the baseline of 5000 identical programs, as they were
%   consistently placed in just 1 cluster.}
% \label{fig:diversity-baseline}
% \end{figure}

% \begin{figure}
% \foreach \method in {single, complete, average}
% {
% \begin{tikzpicture}
%   \begin{axis}[
%       xlabel=Threshold,
%       ylabel=\# of clusters,
%     %   ytick distance=1000,
%       title=Linkage: \method
%     ]
%     \foreach \language in {ocaml, python}
%     {
%       \addplot+[mark=none] table[x=threshold, y=\language_cluster_counts_\method, col sep=comma]{cluster_counts.csv};
%     }
% \legend{OCaml, Python}
% \end{axis}
% \end{tikzpicture}
% }
% \caption{Number of clusters when data is clustered by single, complete, and average linkage.
%   A linkage tree is built using agglomerative clustering, and the resulting tree is broken up
%   into clusters by thresholding the inconsistency coefficient. The inconsistency coefficient
%   of a link is its distance, z-scored against other nearby links.}
% \label{fig:diversity}
% \end{figure}

\begin{figure}
\begin{tikzpicture}
  \begin{axis}[
      xlabel=Clustering threshold parameter,
      ylabel=\# of naturally occuring program clusters,
      xmax=1.25,
      legend style={at={(0.5,0.25)},anchor=north}
    %   ytick distance=1000,
    %   title=Linkage: single
    ]
    \foreach \language in {ocaml, python}
    {
      \addplot+[mark=none] table[x=threshold, y=\language_cluster_counts_single, col sep=comma]{cluster_counts.csv};
    }
\legend{NATE OCaml data, Our Python data}
\end{axis}
\end{tikzpicture}
\caption{Number of natural program clusters in each dataset as a function
of clustering parameter (informally, how ``strict'' or ``tight'' the
clustering is). \fixme{Talk about how you sample to be fair to the smaller
datasets.} At all points our dataset demonstrates more diversity than
that of previous work.}
\label{fig-diversity}
\end{figure}

To demonstrate that our evaluation dataset is not only larger but also more
diverse than those used in previous work, we compare the diversity of
programs used here to those in a relevant baseline. The Nate
algorithm~\cite{learning-to-blame} also provides type error localization
and also focuses on a machine-learning approach backed by slicing. However,
Nate targets strongly statically typed OCaml programs. Nate's evaluation
also focused on beginner-written programs: specifically, it considered
student submissions to 23 different university homework problems.  Such a
dataset is comparatively homogeneous, raising concerns about whether
associated evaluation results would generalize to more diverse settings.

In our PythonTutor dataset, by comparison, users were not constrained to
specific university assignments. We hypothesize that the evaluation data are
more heterogeneous. To assess this quantitatively, we used agglomerative
clustering to discover the number of ``natural'' program categories
present in both our dataset and also in the Nate dataset. Datasets with
more distinct program categories are more heterogeneous.

% The exact number of clusters returned
% depends on the value we choose for the algorithm's threshold parameter, so we
% checked all thresholds at .01 intervals, starting at 0 and ending at the point
% where everything collapses into a single cluster. As shown in
% \figref{diversity}, regardless of the choice of threshold, our data breaks into
% more clusters than the NATE data. This confirms that our data is more
% heterogeneous.

% To cluster programs, we transform the program ASTs into flat strings, and
% then perform agglomerative clustering on them, with string edit distance
% as a distance metric.

\subsubsection{Transformation and Distance Metric}

Many clustering algorithms depend on distance metrics.
To measure the distance between programs, we flattened their ASTs into
strings of tokens, and then computed the Levenshtein edit
distance~\cite{levenshtein}. The decision to flatten the tree rather than
compute a distance directly on the ASTs relates to scalability for our
large datasets. The state of the art in tree edit distance has cubic
asymptotic complexity~\cite{PAWLIK2016157}, while the Levenshtein distance
can be computed in time quadratic in the length of the
strings~\cite{lev-quadratic}. We did implement and attempt a tree edit
distance metric, but it did not terminate for this analysis in a practical
amount of time.

The flattening process, which is based on standard approaches for
tree-structured data~\cite{dist-site}, is defined recursively: the string
for an AST begins with a unique token corresponding to the AST node type,
which is followed by the concatenation of the transformations of the tree's
subtrees.  The string ends with an ``end'' token to denote the end of the
tree. This embeds information related to the tree structure of the AST in
the flattened string.

\subsubsection{Clustering Algorithm}

We performed agglomerative clustering on the
datasets of flattened programs~\cite{modern-clustering}.
This is a method of clustering in which every datapoint starts in its own
cluster, and the two closest clusters are merged every step of the algorithm,
until every datapoint is in the same cluster. This produces a binary tree, where
each internal node corresponds to a cluster formed by merging two other clusters,
and each leaf corresponds to an item in the dataset. We used a single linkage
approach in which the distance between clusters of multiple items was considered
to be the minimum distance between their elements.

To produce a cluster count from the agglomerative tree, we followed the
standard approach of splitting the tree at internal nodes where the two
joined clusters are more distant than some threshold. To account for
potential programming languages differences between Python and OCaml (such
as expressivity), we z-score nodes against not only others at the same tree
depth, but also against against others one or two levels
below~\cite{inconsistent}. This makes cluster counts at each threshold
value comparable. Our implementation uses the standard
SciPy library (e.g., \lt{scipy.cluster.hierarchy.fcluster} with the
\lt{inconsistent} method).

\fixme{We need to say something about how we controlled for number, lest
reviewers think we have more clusters simply because we have hundreds of
thousands of datapoints.}

\figref{diversity} shows the results of clustering on our dataset and the
Nate dataset. At a high level, the structural similarity in the graphs
confirms the comparability of cluster counts between the two datasets.  For
all values of the inconsistency threshold, there are more clusters in our
Python dataset than in the baseline OCaml dataset. This suggests that our
dataset contains a more diverse set of programs. Note that we are not
claiming any novelty or advances in clustering accuracy in this
determination (indeed, scalability concerns limited us to coarser
approaches); instead, our claim is that even with simple clustering,
it is clear that our dataset contains a greater diversity of programs, even
when controlling for size, than were considered by previously-published
evaluations. We view it as an advantage of our algorithm that it can apply
to multiple different program categories.

% \subsection{RQ 5 --- Wishlist: Partitioning Kinds of Errors}

% Can we partition the kinds of errors that people make? What are common
% misconceptions? Do these correspond to paths of decision trees.

% If we don't get to the source-sink thing, it could go here.

% I don't think we have time, in this paper, to actually implement a
% two-level machine learning.

\subsection{Qualitative Analysis}

%%% Original for fig-win-example-1; index 19615 of v4
% def word_checker(food):
%     w_alphabet_ned = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',
%                 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',
%                 'u', 'v', 'w', 'x', 'y', 'z', 'Ã«', 'Ã¯', 'Ã©', 'Ã¨', 'Ã¶', 'Ãª',
%                 'Ã¼', 'Ã³', 'Ã§', 'Ã¡', 'Ã ', 'Ã¤', 'Ã»', 'Ã®', 'Ã­', 'Ã´', 'Ãº', 'Ã±', 'Ã¢']
%
%     w_list_food = list(food)
%
%     for letters in w_list_food:
%         if letters not in w_alphabet_ned:
%             w_list_food = w_list_food.remove(letters)
%         food = ''.join(w_list_food)
%     print(food)
%
%
%
%
% word_checker('sl^* ( ) 0a')

\begin{figure}
\begin{lstlisting}
def devowel(word):
    word = 'foo'
    w_list = list(word)
    vowels = ['a', 'e', 'i', 'o', 'u']
    for letter in w_list:
        if letter not in vowels:
            w_list = w_list.remove(letter)
    word = ''.join(w_list)
print(devowel('foo'))
\end{lstlisting}
\caption{A program with a bug on line 7.}
\label{fig-win-example-one}
\end{figure}

%%% Original for fig-win-example-2; index 50219 of v4
% GeorgiaAreaCodes = [404, 770, 678, 706, 762, 229, 423, 912, 478]
% for i in range(0, len(GeorgiaAreaCodes) + 1):
%     print(GeorgiaAreaCodes[i])

We now highlight a few indicative localization examples in detail to
demonstrate how our algorithm accurately localizes faults. 
These examples are simplified slightly for presentation and to protect the
anonymity of the programmers, but they retain the essential character of
the originals from our dataset.

\paragraph{NoneType}
The function in \figref{win-example-one} attempts to remove all vowels from
a given word. However the \lt{remove} method in line 7 actually modifies
the list in place and returns \lt{None}. By contrast, the user-corrected
version forgoes the assignment instead uses \lt{w_list.remove(letter)}. In
the buggy case, Python does not crash until line 8, where its message is
the somewhat-misleading \lt{TypeError: can only join an iterable}. However,
our decision tree flags the correct statement, based on the features 
that it is an assignment statement whose second child has type
\lt{NoneType}. This seems to capture the intuitive reasoning that there is
rarely a good reason to assign the value \lt{None} to a variable. 

Note that our algorithm uses both the syntactic form of the statement (an
assignment) and the dynamic type of one of its children, so all our
categories of features --- syntactic, dynamic, and contextual --- were
useful here.

\begin{figure}
\begin{lstlisting}
areaCodes = [800, 555]
for i in range(0, len(areaCodes) + 1):
    print(areaCodes[i])
\end{lstlisting}
\caption{A program with an off-by-one bug on line 2.}
\label{fig-win-example-two}
\end{figure}

\paragraph{Off-by-one bugs}
In \figref{win-example-two}, the programmer incorrectly adds one to the high
end of their range, causing an \lt{IndexError: list index out of range} on line
3 during the final iteration of the for loop. The correct expression to blame
is the addition \lt{len(areaCodes) + 1}. Some of the features that our
decision tree uses to correctly localize this bug include that the error is an
index error, the type of the parent, and the fact that it is an addition
expression. This example also highlights the use of all three categories of
features simultaneously to capture a notion of root cause that more closely
aligns with human expectations.

\subsection{Threats to validity}
\label{sec-threats}

Although our evaluation demonstrates that our algorithm scales to
accurately localize Python errors in lage datasets, our results may not
generalize to all use cases. 

\emph{Overfitting.} Since our algorithm makes use of supervised machine
learning, one threat to validity is overfitting (i.e., learning a model
that is too complex with respect to the data and thus fails to generalize).
We mitigate this threat by using cross-validation, as discussed in
\secref{training}. 

\emph{Language choice.} We have only demonstrated that our technique works
for Python 3. We hypothesize that it should apply to similar
dynamically-typed languages, such as Ruby, but such evaluations remain
future work. We mitigate this threat slightly by constructing our algorithm 
so that it does not depend on Python-specific features or node types
(e.g., in \secref{slice-algorithm} we explicitly omit relatively ``exotic'' 
features such as generators, ``yield'' and coroutines that may not always
be present in other languages).

\emph{Target population.} Unlike many other studies that focus specifically
on students, we have very little information about the makeup of our
subject population. The general popularity of PythonTutor is an advantage
for collecting a large, indicative dataset, but it does mean that we have
no specific information about the programmers or the programs they were
trying to write. In general the website targets beginners rather than
professional software engineers. As a result, our technique may not
generalize well to the kinds of bugs that would be found in an industrial
setting rather than a novice or educational one.

\emph{Ground truth.} The size of our dataset precluded the manual annotation 
of each buggy program. Instead, we used historical successful
edits from actual users as our ground truth notion of the desired fault
localization. This has the advantage of aligning our algorithm with user
intuitions in cases where there are multiple logically-consistent answers
(see \secref{motex}), and thus increasing the utility of our tool. However,
this definition of ground truth may be overly permissive: the next
correct program in the historical sequence may contain additional spurious
changes beyond those strictly needed to fix the bug.
We mitigate this problem by discarding as outliers program pairs that have
changed too much. \fixme{Really? This is the first time WRW is hearing
about this. If this is true, you have to go back and mention this earlier
where we talk about ground truth definitions and data set selection.} 

\subsection{Evaluation Summary}

We evaluated our results on over 270,000 pairs of beginner-written Python
programs. Our primary research result relates to accuracy: 
\begin{framed}
\noindent Our algorithm is very accurate, implicating the correct program
terms 60--79\% of the time (for Top-1 to Top-3 lists), compared to the
Python interpreter's 45\% accuracy. 
\end{framed} 
In addition, we investigated the design decisions of our algorithm:
\begin{framed}
\noindent All feature categories (i.e., static, dynamic, and contextual)
were critical, as measured by Gini importance, ANOVA F-score
and a leave-one-out analysis. 
\end{framed}
Finally, we investigated the sensitivity of our algorithm to various
contexts:
\begin{framed}
\noindent Our algorithm generalizes, with similar accuracy across the top
error types and on a dataset that is more diverse than that of related
work.
\end{framed} 



\section{Related Work}
\fixme{This must all be rewritten or dropped, as it is directly copied from
my previous papers. It serves as a placeholder to indicate length and to
remind us of some Software Engineering bits we might cite.}

There are several fault localization techniques that rely much more heavily
on dynamic information\footnote{Note: we do not directly compare to these
state-of-the-art fault localization techniques for several reasons.  First,
the benchmarks used in this paper do not always have available the test
suites needed by dynamic techniques and, conversely, those used in the
previous work do not have the publicly available bug reports required
by our technique.  Additionally, we return an answer only when confident in
our result set and also employ a top list of all available results, while
previous work is typically evaluated using a score metric.  We consider the
techniques complimentary.}~\cite
{harrold05,Renieris03,cleve05,wang09}.
In general,
these techniques leverage differences between passing and failing program
executions.  While effective, this type of approach requires program traces
for not only the specific fault at hand but also from a comprehensive regression
test suite for comparison.
A hybrid approach that considers both dynamic
traces and static natural language similarity was recently proposed by
Medini \textit{et al.}~\cite{Medini11}.
In contrast to these approaches, our technique relies purely on static
information that is readily available in most commercial systems with minimal
additional developer effort.

Prabhakararao and Ruthruff \emph{et al.} performed two human studies
to gauge the effectiveness of an interactive fault localization tool
developed for end users with little to no
experience~\cite{Prabhakararao03,ruthruff05}.  The goal of their
studies was to evaluate the use of feedback when locating faults and
to generally study the process of fault localization, especially by
users with no expert domain knowledge of the source.  By comparison,
our human study also examines the fault localization process but for
the purpose of evaluating software quality metrics.  We are less
interested in the specific process and more concerned with the
resulting accuracy and the human intuitions about the code in
question.  Additionally, our human study is of a much broader scope and
thus we hope it is more generalizable.
%Additionally, their studies included 10 and 20
%participants respectively using a spreadsheet-based visualization tool.
%Our study of 61 humans looking at actual source code potentially provides
%more generalizable results and targets specifically the area of software
%maintenance completed by trained programmers.


Ashok \textit{et al.} propose a similar natural language search technique
in which users can match an incoming report to previous reports,
programmers and source code~\cite{Ashok09}.  By comparison, our technique
is more lightweight and focuses only on searching the code and the
defect report.

Jones \textit{et al.} developed Tarantula, a technique that performs
fault localization based on the insight that statements executed often
during failed test cases likely account for potential fault
locations~\cite{harrold05}. Their approach is quite effective when
a rich, indicative test suite is available and can be run as part of
the fault localization process. It thus requires the fault-inducing
input but not any natural language defect report. By contrast,
our approach is lightweight, does not require an indicative test
suite or fault-inducing input, but does require a natural language
defect report. Both approaches will yield comparable performance, and
could even be used in tandem.

Cleve and Zeller localize faults by finding differences between
correct and failing program execution states, limiting the scope of
their search to only variables and values of interest to the fault in
question~\cite{cleve05}. Notably, they focus on those variable and
values that are relevant to the failure and to those program execution
points where transitions occur and those variables become causes of
failure. Their approach is in a strong sense finer-grained than ours:
while nothing prevents our technique from being applied at the level
of methods instead of files, their technique can give very precise
information such as ``the transition to failure happened when $x$
became 2.'' Our approach is lighter-weight and does not require
that the program be run, but it does require defect reports.

Renieris and Rice use a ``nearest neighbor'' technique in their
Whither tool to identify faults based on exposing differences in
faulty and non-faulty runs that take very similar executions
paths~\cite{Renieris03}. They assume a large number of correct runs
(e.g., normal test cases) and one failing run. Their approach uses a
distance criterion to select the correct run that is closest to the
failing run and produces a report of ``suspicious'' parts of the
program. By comparison, we chose to limit the programmatic information
used by our technique to only that which was reported by users: we
do not use test case runs but do need natural language.

Liblit \textit{et al.} use Cooperative Bug Isolation, a statistical
approach to isolate multiple bugs within a program given a deployed user
base. By analyzing large amounts of collected execution data from real
users, they can successfully differentiate between different causes of
faults in failing software~\cite{liblit05}. Their technique produces
a ranked list of very specific fault localizations (e.g., ``the fault
occurs when $i > arrayLen$ on line 57''). In general, their technique
can produce more precise results than ours, but it requires a set of
deployed users and works best on those bugs experienced by many users.
By contrast, we do not require that the program be runnable, much less
deployed, and use only natural language defect report text.

\section{Conclusion}

\bibliographystyle{abbrv}
\bibliography{nanomaly,sw,temp,slice,cluster,ml}

\end{document}
