% 10+2 Due Fri 28 Aug anywhere-on-earth
%
% ICSE 2019 Technical Track submission must not exceed 10 pages,
% including all text, figures, tables, and appendices; two additional pages
% containing only references are permitted.
%
% ICSE 2019 Technical Track will employ a double-blind review process.
% Thus, no submission may reveal its authorsâ€™ identities.
%
% https://2019.icse-conferences.org/track/icse-2019-Technical-Papers#Call-for-Papers

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{pgfplots}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

%
\title{Blaming the Typeless: \\ Scalable, Human-Centric Python Localization}

\iffalse

\author{\IEEEauthorblockN{Benjamin Cosman}
\IEEEauthorblockA{\textit{UC San Diego}\\
blcosman@eng.ecsd.edu}
\and
\IEEEauthorblockN{Leon Medvinsky}
\IEEEauthorblockA{\textit{UC San Diego}\\
lmedvinsky@eng.ecsd.edu}
\and
\IEEEauthorblockN{Ranjit Jhala}
\IEEEauthorblockA{\textit{UC San Diego}\\
jhala@cs.ecsd.edu}
\and
\IEEEauthorblockN{Westley Weimer}
\IEEEauthorblockA{\textit{University of Michigan}\\
weimerw@umich.edu}
}

\fi

\author{\IEEEauthorblockN{\emph{submitted for double-blind review}}}

\maketitle

\begin{abstract}
Abstract
\end{abstract}

\section{Introduction}

\emph{Placeholder Outline.} This text is an informal placeholder outline
only.

\emph{Identify an important problem.} Support the claim that Python fault
localization for possibly-ill-typed student fragments is an important
problem. Likely steps: fault localization is important, Python is
important, incomplete code is important. First, software engineering is
important and expensive; testing, maintenance, and debugging are the
dominant activities in SE; localization and triage are key steps. Second,
Python is increasingly used for pedagogy and industrial deployment, here
are some GitHub numbers about the rate of growth in Python projects
compared to C or Java, etc. Third, IDE tool use requires operating on code
fragments, here are some numbers from PythonTutor~\cite{Guo2013-vu}, generality, etc.

\emph{Identify the properties of a good solution.} Pick your favorite
three. Some candidates: accuracy (cite Parnin/Orso paper an indicate that
it must be found within top three, etc., since humans will not read long
lists), generality (must apply to student code and incomplete fragments),
scalability (must apply to hundreds of thousands of fragments, must present
real-time responses suitable for web/IDE use), agreement with subjective
judgments (must handle ``ambiguity'' the same way humans would when you
don't know whether to blame the use or the definition, etc.), efficiency
(must operate using a small number of dynamic runs, unlike Tarantula,
etc.).

\emph{Show that the current state of the art is inadequate.} For each $x$ of
\{manual debugging, Python's default error messages,
Mycroft/Sherrloc/Tarantula/whatever\}, show that $x$ lacks at least one of the
desired properties listed above.

\emph{Insights and special sauce.} What are the two or three good ideas
that we bring together to solve this problem? Some candidates: the use of
modern machine learning techniques (deep learning, etc.); the use of
tree-based contextual features to hit the sweet spot between syntax and semantics;
special handling of unsatisfied use-def type constraints (TBA); etc.

\emph{What is our proposal?} We propose XYZ, an algorithm to do ABC. It
takes as input DEF. It has stages P, Q and R. It produces as output GHI. It
provides guarantees JKL.

\emph{How will we evaluate it?} For each of the properties of a good
solution listed above, indicate how we will evaluate our contribution. What
evidence will we provide? Typically, for each property $x$, we support our
claim to $x$ with either a formal proof (e.g., our algorithm has $x$ by
construction), an empirical evaluation on software artifacts, or an
empirical evaluation on human subjects. In each case, indicate both the
evaluation metrics (e.g., if we care about efficiency, is it measured in
lines-per-second? if we care about precision, what is our non-standard
top-three metric) and success criteria (e.g., how will we define success?
what is the baseline we want to beat). For empirical evaluations, briefly
and implicitly argue that the results are likely to generalize by giving
the size of the sampled set.

\emph{Introduction wrap-up.} The contributions of this paper are as follows
...

\section{Motivating Example}

\begin{figure}
\begin{verbatim}
year = int(time.strftime("%Y"))
age = input("Enter your age")
print("You will be twice as old in:")
print(year + age)
\end{verbatim}
\caption{A buggy program}
\end{figure}

Consider the program in Fig. 1, adapted from our dataset. The program
attempts to use the current year and the user's age to compute their
birth year and the year they can vote. However, while line 1 correctly
turns the current year from a string into an int, this `int' call is
missing from line 2 so `age' remains a string, and thus the addition
on line 4 causes the program to crash.

Notice that while a correct fix (and the one that the programmer
actually used) is to add an `int' call to line 2, another possible ``fix''
would be to \textit{remove} the `int' call from line 1 - both year and
age would then be strings, and the addition on line 4 would be interpreted
as string concatenation. This shows the necessity of a heuristic, data-driven
approach, since without test cases there is no way to tell for sure which
fix is better.

We can also start to see some features which will be
important for finding bugs: we will need the \textit{type} of age, as
well as some \textit{contextual} information about how it's used in the program.

% TODO: would also be nice if the example showed that the place Python crashes
% may not be a place that should be fixed?

\section{Overview and Approach}

We present an algorithm for accurately localizing faults~\cite{tarantula} in
dynamically-typed Python programs that exhibit non-trivial uncaught runtime
exceptions. We do not consider syntax errors or references to undefined
variables. Our algorithm uses machine learning models based on static and
dynamic features to implicate suspicious expressions. Since studies have
found that voluminous fault localization output is not useful to
developers~\cite{orso-parnin,orso-parnin2015}, we focus on producing
Top-1 and Top-3 rankings.

Our algorithm first extracts static and dynamic features from Python
program (Section~\ref{sec-features}). Next, using a labeled training
corpus, we learn a machine learning model over those features
(Section~\ref{sec-model}). Once the model has been learned, we localize
faults in new Python programs by extracting their features and applying the
model.

\subsection{Feature Extraction}
\label{sec-features}

Our key insight is that FIXME-INSIGHT. Are a result, we make use of
static, dynamic and contextual information. Static features, such as FIXME,
are effective at capturing FIXME-INFORMATION. By contrast, dynamic
features, such as FIXME, support the modeling of FIXME. Finally, we also
include contextual information, such as FIXME, which allows our algorithm
to accurately handle FIXME. We consider the empirical justification of
these features in Section~\ref{sec-eval}.

We calculate features for each statement and expression in the program.

\begin{itemize}

\item \emph{Static / syntactic features}
\begin{enumerate}
    \item What kind of statement/expression it is, e.g. assignment / return
     / import (for statements) or variable / literal / application (expressions).
    \item Size (the number of AST nodes in the subtree rooted at this node)
\end{enumerate}

\item \emph{Dynamic features}
We run each program through an instrumented interpreter (i.e. a modified PythonTutor
backend, which itself is based on the BDB debugging library). This lets us
compute the following features:
\begin{enumerate}
    \item What type(s) does the expression have at runtime? (The values of this feature
    can be a type like int, or the special values "unknown" if the expression is
    never successfully evaluated and "multiple" if it changes type.)
    \item Is it part of the error slice?
    \item Is it the node at which the program actually crashes?
\end{enumerate}
(Since this interpreter works at the granularity of lines and we want to work with
expressions as well, we first convert each program to ANF.)

We compute the error slice by running the program and building a dependency
graph where node A depends on node B if either B defines a variable that A uses,
or B affects control flow allowing A to run.

We also check what uncaught exception is thrown by the program (TypeError,
KeyError, etc), and add that as a constant feature to all vectors derived from that program.

\item \emph{Contextual features} After all other features are computed so we have a
vector $v_e$ representing expression $e$, we set $v_e^{+context} = v_e \circ
v_p \circ v_{c_1} \circ v_{c_2} \circ v_{c_3}$ where $p$ is the expression's parent
in the AST and $c_i$ are its children. This allows us to recover some of the
program structure that would otherwise be completely lost when we convert the
structured program into an unstructured bag of feature vectors.

\item \emph{Label} Each feature vector is labeled by whether that expression/statement
changed between the crashing and fixed versions. (This is our proxy for whether
that node is to blame for the crash).

\end{itemize}

Each vector is then one-hot encoded (e.g. feature "Type : Int" becomes set
of features "Type-Int : 1", "Type-Bool : 0", "Type-Str : 0", etc.) for use with
standard machine learning tools.

\subsection{Machine Learning and Modeling}
\label{sec-model}

The models we train (inc. decision trees/forests,
MLP, GBM) can then rank expressions of a test program in order of how likely they
are to be the source of the bug, so we can compute Top-k accuracies: how often
is the true program change located in the model's first k outputs.

\section{Approach, in more detail}

We follow the overall strategy of \cite{learning-to-blame} and represent each
buggy program by a Bag of Abstracted Terms. In our setting, a `term' means either
a statment or an expression. For each term in the program we compute the following features:

\subsection{Dynamic features}

We first convert each buggy program to an equivalent program that is closer
to ANF, then run it through the PythonTutor backend. This backend is based on the
Python debigging library BDB; it returns to us a trace of the program along with
the state of the heap at each execution step. We use this information to compute
the following features for each term:

\emph{Type} The possible values for the Type feature include all the basic Python
types (such as int and tuple) as well as a few special values:
\begin{itemize}
    \item IsStatement - Given to all statements, since our terms include both
    statments and expressions but only expressions actually have types.
    \item Unknown - Given to expressions that are never evaluated in the dynamic
    trace.
    \item Multiple - Given to expressions which are evaluated multiple times in
    a trace and do not always have the same type.
\end{itemize}

\emph{Slice} (TODO - Leon can you expand this? What did you base our slicing on?)
The goal of this feature is to help eliminate terms that cannot be the source of
the crash. These include lines that are never run, lines that run successfully
and don't affect any variables that are used later, and so on. More specifically, we
build up a dependency graph of all lines in the program trace. Edges in the graph
represent either def-use dependence or control-flow dependence. Then ideally,
any node in the graph that is not a transitive dependency of the line that crashes
can't be responsible for the crash. However dynamic slicing is not perfect (TODO
include example?), so unlike in prior work, we do not treat membership in the slice
as a hard constraint.

\emph{Crash location} As discussed later in the evaluation, the precise term that
causes Python to crash is in fact the one the user decides to change about 30\% of
the time, so we use it as a feature (and expect it to be a high-signal one, too.)

\emph{Exception type} The type of error thrown by the program is useful context for
finding the error. For example, if asked how likely a random `0` is of being a bug, you'd give
a higher answer if you knew that the program eventually crashes with a division by
zero exception. Thus we attach the exception type as a feature to \emph{every}
vector derived from that program.

\subsection{Contextual features}

The approach of representing a program by a Bag of Abstracted Terms, if implemented
naively, runs into the problem that the terms lose their context in the program
as a whole. For example, the features for the term `0` include its type (int) and
syntactic description (int literal), but there is no way that's enough to decide
if it's innocuous, like the 0 in `x = 0`, or clearly the source of a bug, like the
0 in `x / 0`. To restore some of this context, we introduce contextual features.
This involves duplicating all our other features four times - once for the parent
and once each for a term's first three children in the AST. So since `x / 0` has
the feature SyntacticForm = DivisionOp, the `0` of `x / 0` has the feature
ParentSyntacticForm = DivisionOp. Similarly, since `0` has SyntacticForm = IntLiteral,
`x / 0` has Child1SyntacticForm = IntLiteral. In order to make all feature vectors
the same length, nodes that have no parent or fewer than three children have the
relevant features filled out with a special NotApplicable value.

\subsection{Static features}

\emph{Syntactic form} The first feature we consider is what kind of node the term
is in the AST. For statements this has values like Return or Import; for expressions
Variable or Application.

\emph{Size} We also count the number of descendent nodes in the subtree rooted at that node.

\subsection{Machine learning}

\emph{Pairing programs} For each PythonTutor user, we look at the sequence of
programs they attempted to run. Each program that crashes is paired with the next
program by that user which does not crash. Many programs do not make it to the
end of our data pipeline; here are the most common reasons:
\begin{itemize}
    \item The user submitted only programs that crash or only programs that do
    not crash, so we can't create any pairs for that user.
    \item The program is out of scope for PythonTutor. For
    example, the program imports forbidden libraries (like os), or the program
    runs in over 1000 execution steps.
    \item The proportion of the program that has changed between the buggy and
    fixed versions is more than a standard deviation above the mean.
\end{itemize}
We end up with \textasciitilde270,000 usable pairs of programs.

\emph{Training methodology} At this point we have feature vectors describing every term of every program in
our dataset. The vectors from a random 30\% of the program pairs are set aside for testing and
the other 70\% are used for training. Within the training data, we use 3-fold
cross-validation to select the best parameters for our models, before training
using those parameters on the full training set and then scoring the models
using the testing set. Each trained model takes in any vector representing a term
in a buggy program, and returns a confidence score representing how likely it is that
that term was one of the terms changed between the fixed and buggy versions.
Thus we can treat the model as providing a \emph{ranking} over all terms by
confidence, where the top result is the one that the model deems most likely
to be changed. For
a given k, we score the model based on Top-k accuracy: for what proportion of the
programs is a correct answer (i.e. a term that is actually changed) present in the
top k results. We retrain the models for each value of k, since a model optimized
to produce the single best result might be different from one optimized to get a
good result into the top three.

In each case the most effective model we trained was a decision tree. (The other
models we tried were Logistic Regression and MLP. Yaoyuan - anything to add to
this section?)

\section{Evaluation}
\label{sec-eval}

We conducted a large-scale empirical evaluation of our algorithm with the
aim of addressing a number of research questions:
\begin{enumerate}

\item[RQ1]{Can we accurately localize non-trivial faults in Python
programs?}

\item[RQ2]{Which features are the most important for Python fault
localization overall?}

\item[RQ3]{Which features are the most important for various categories of
Python defects?}

\item[RQ4]{Is our algorithm accurate on heterogeneous sets of programs?}

\item[RQ5]{FIXME clustering?}

\end{enumerate}

\subsection{Data Set}

Our raw data consist of every Python 3 program that a user executed on
PythonTutor.com~\cite{Guo2013-vu} (not in ``live'' mode) during 2017, other
than those with syntax errors or undefined variables.  Each program which
crashes (throws an uncaught Python exception) is paired with the next
program (by the same user) that does not crash, under the assumption that
the latter is the fixed version of the former. We discard pairs where the
difference between crashing and fixed versions is too high (more than a
stddev above average), since these are most likely to be violations of that
assumption (i.e., the program that does not crash is unrelated to the
crashing program).

In our evaluation, we used FIXME as a training set and FIXME as a held-out
evaluation set. We employed cross-validation~\cite{kohavi} to help address
the potential threat of overfitting.

\subsection{Methodology}

For these evaluations, we employ machine learning algorithm FIXME, which
provided the best balance of FIXME and FIXME (time? accuracy?) on our
training set.

We report Top-1 and Top-3...

We define FIXME-ACCURACY to be FIXME.

\subsection{RQ 1 --- Fault Localization Accuracy}

\begin{figure}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    symbolic x coords={Top-1,Top-3},
    xtick=data,
    ymin=0,
    ylabel=Model Accuracy,
    enlarge x limits=0.5,
    legend style={at={(0.5,-0.15)},anchor=north}
]
    \addplot table[x=scoreName, y=withEverything, col sep=comma]{fault-localization.csv};
    \addplot table[x=scoreName, y=baseline, col sep=comma]{fault-localization.csv};
\legend{Our best model,Python interpreter}
\end{axis}
\end{tikzpicture}
\caption{Based on a decision tree on the largest
dataset (272534 pairs). TODO: also train for Top-2}
\label{fig:full-dataset-acc}
\end{figure}

\begin{figure}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    symbolic x coords={Top-1,Top-3},
    xtick=data,
    ymin=0,
    ylabel=Model Accuracy,
    enlarge x limits=0.5,
    legend style={at={(0.5,-0.15)},anchor=north}
]
    \addplot table[x=scoreName, y=all, col sep=comma]{removing-features.csv};
    \addplot table[x=scoreName, y=noTypes, col sep=comma]{removing-features.csv};
    \addplot table[x=scoreName, y=noContext, col sep=comma]{removing-features.csv};
    \addplot table[x=scoreName, y=noSlice, col sep=comma]{removing-features.csv};
\legend{All Features,No Types, No Context, No Slicing}
\end{axis}
\end{tikzpicture}
\caption{Accuracy when features are removed, based on decision trees on a random subset of 20000 pairs.
TODO: train better model and/or
train on greater fraction of the data? TODO: Top-2. NOTE: removing slicing makes basically no difference...}
\label{fig:removing-features}
\end{figure}

prereq: define accuracy

Goal: Establish that our algorithm works. This is the most basic question,
but also the most relevant. If the reader takes away nothing else, they
should conclude that our algorithm is effective.

PLOT: See Figs. \ref{fig:full-dataset-acc} and \ref{fig:removing-features}

\subsection{RQ 2 --- Feature Predictive Power}

\begin{table}[]
\begin{tabular}{l|l|l}
Feature Name & Feature Category & Feature Predictive Power \\ \hline
F-Size-P & Contextual (Syntactic) & 0.112624 \\
F-Stmt-Constr-C3= & Contextual (Syntactic) & 0.061552 \\
F-Expr-Constr-P=List & Contextual (Syntactic) & 0.054632 \\
F-PythonBlame & Dynamic (Error location) & 0.039189 \\
F-Type=unknown & Dynamic (Type) & 0.037390 \\
F-Type-P=unknown & Contextual (Type) & 0.037330 \\
F-PythonMsg=IndexError & Dynamic (Error message) & 0.032749 \\
F-Size & Syntactic & 0.028203 \\
F-Expr-Constr-P=Dictionary & Contextual (Syntactic) & 0.026663 \\
F-Size-C1 & Contextual (Syntactic) & 0.023662 \\
F-Expr-Constr=Var & Syntactic & 0.020390 \\
F-Type-C1=unknown & Contextual (Type) & 0.019815 \\
\end{tabular}
\caption{Based on the Top-3 Decision Tree on the full dataset. Feature Predictive Power here is the Gini Importance. TODO: add
better-known measure like ANOVA or ReliefF.}
\label{tab:feature-predictive-power}
\end{table}

prereq: define predictive power (leave-one-out, leave-one-in, Relief-F,
ANOVA, whatever)

PLOT: See Table \ref{tab:feature-predictive-power}

Use as a rough guide: Table 3 on Page 8 of
https://web.eecs.umich.edu/~weimerw/p/weimer-icsm2010.pdf

Goal: Establish that we are smart for including all of these features. Note
which individual features or feature categories were (not) included in
previous work.

Give a simple narrative about our effectiveness: implicitly, we were smart
for deciding to include these features, better ingredients make better
results, we have great results.

TODO: what about turning on/off slicing?

TODO: what about turning on/off types? (is that separate from slicing?)

\subsection{RQ 3 --- Defect Categories}

prereq: define defect category. This could be either the raw python
exception name or it could be clusters that we have manually created.
Ranjit notes: result could be ``we need everything for everything'', at
which point this is useless and should be skipped. Per Ben's exam: may find
that for different categories of errors we should use different ML
classifiers.

PLOT Type: Bar Graph (three bars per point on the X axis)

PLOT Y Axis: Leave-One-In Accuracy (normalized per X axis point)
Ranjit notes: on smaller subsets we could rebuild the classifier each time.
Wes notes that if we do, it takes us longer. If we re-use the monolithic
model we can do this very quickly.

PLOT X AXis: Defect Categories

PLOT Bars: Static, Dynamic, Context

Goal: Demonstrate that each category of feature is essential for a certain
piece of the problem. We need ABC feature to handle DEF class of defects,
but ABC feature does not work well on GHI class of defects, for those we
need JKL features. Implicit: we were smart for including all of these
categories.

\subsection{RQ 4 --- Diversity of Programs}

In Nate there were 20 different functions. All the programs were doing the
same 20 functions.

Here, people are doing anything they want. Random stuff going on over here.

We want to show that we get high accuracy *over a heterogenous set of
programs*. Leon?

prereq: cluster the programs based on similarity. Using agglomerative
clustering plus python edit distance. How good the performance will be?
right now it creates a clustering tree. binary tree, based on min distance.
can give it a threshhold for distance (elements w/in a group). don't forget
to rename all variables to X, perhaps normalize (edit distance size).

leon: could also cluster by inconsistency coefficient

PLOT Type: line graph (standard plot)

PLOT X AXIS: clustering distance parameter

PLOT Y AXIS: number of different clusters found

PLOT LINES: line 1 is ``Nate programs'', line 2 is ``25\%, at random, of our dataset''

Goal: show that ``no matter how you decide program differences'', our
dataset is always more diverse/heterogeneous than is previous work. So our
success is amazing.

\begin{figure}
\foreach \method in {single, complete, average}
{
\begin{tikzpicture}
  \begin{axis}[
      xlabel=Threshold,
      ylabel=\# of clusters,
    %   ytick distance=2,
      title=Linkage: \method
    ]
    \addplot+[mark=none] table[x=threshold, y=python_twenty_cluster_counts_\method, col sep=comma]{cluster_counts.csv};
\end{axis}
\end{tikzpicture}
}
\caption{The number of clusters when clustering a baseline of twenty dissimilar programs from
  Rosetta Code. We give no plot for the baseline of 5000 identical programs, as they were
  consistently placed in just 1 cluster.}
\label{fig:diversity-baseline}
\end{figure}

\begin{figure}
\foreach \method in {single, complete, average}
{
\begin{tikzpicture}
  \begin{axis}[
      xlabel=Threshold,
      ylabel=\# of clusters,
    %   ytick distance=1000,
      title=Linkage: \method
    ]
    \foreach \language in {ocaml, python}
    {
      \addplot+[mark=none] table[x=threshold, y=\language_cluster_counts_\method, col sep=comma]{cluster_counts.csv};
    }
\legend{OCaml, Python}
\end{axis}
\end{tikzpicture}
}
\caption{Number of clusters when data is clustered by single, complete, and average linkage.
  A linkage tree is built using agglomerative clustering, and the resulting tree is broken up
  into clusters by thresholding the inconsistency coefficient. The inconsistency coefficient
  of a link is its distance, z-scored against other nearby links.}
\label{fig:diversity}
\end{figure}

\subsection{RQ 5 --- Wishlist: Partitioning Kinds of Errors}

Can we partition the kinds of errors that people make? What are common
misconceptions? Do these correspond to paths of decision trees.

If we don't get to the source-sink thing, it could go here.

I don't think we have time, in this paper, to actually implement a
two-level machine learning.

\subsection{Qualitative Analysis}

Pick out one example where we did poorly. Walk through why it is ``out of
scope'', in some sense: we would need features that capture XYZ to handle
it correctly, but those are either expensive or undecideable. Hint at
future work.

Pick out a few examples where we did well and the baseline did poorly. Walk
through how interesting and indicative they are and explain why we did well.

\subsection{Evaluation Summary}

Summarize the evaluation. One sentence per subsection.

\section{Related Work}
FIXME: This must all be rewritten or dropped, as it is directly copied from
my previous papers. It serves as a placeholder to indicate length and to
remind us of some Software Engineering bits we might cite.

There are several fault localization techniques that rely much more heavily
on dynamic information\footnote{Note: we do not directly compare to these
state-of-the-art fault localization techniques for several reasons.  First,
the benchmarks used in this paper do not always have available the test
suites needed by dynamic techniques and, conversely, those used in the
previous work do not have the publicly available bug reports required
by our technique.  Additionally, we return an answer only when confident in
our result set and also employ a top list of all available results, while
previous work is typically evaluated using a score metric.  We consider the
techniques complimentary.}~\cite
{harrold05,Renieris03,cleve05,wang09}.
In general,
these techniques leverage differences between passing and failing program
executions.  While effective, this type of approach requires program traces
for not only the specific fault at hand but also from a comprehensive regression
test suite for comparison.
A hybrid approach that considers both dynamic
traces and static natural language similarity was recently proposed by
Medini \textit{et al.}~\cite{Medini11}.
In contrast to these approaches, our technique relies purely on static
information that is readily available in most commercial systems with minimal
additional developer effort.

Prabhakararao and Ruthruff \emph{et al.} performed two human studies
to gauge the effectiveness of an interactive fault localization tool
developed for end users with little to no
experience~\cite{Prabhakararao03,ruthruff05}.  The goal of their
studies was to evaluate the use of feedback when locating faults and
to generally study the process of fault localization, especially by
users with no expert domain knowledge of the source.  By comparison,
our human study also examines the fault localization process but for
the purpose of evaluating software quality metrics.  We are less
interested in the specific process and more concerned with the
resulting accuracy and the human intuitions about the code in
question.  Additionally, our human study is of a much broader scope and
thus we hope it is more generalizable.
%Additionally, their studies included 10 and 20
%participants respectively using a spreadsheet-based visualization tool.
%Our study of 61 humans looking at actual source code potentially provides
%more generalizable results and targets specifically the area of software
%maintenance completed by trained programmers.


Ashok \textit{et al.} propose a similar natural language search technique
in which users can match an incoming report to previous reports,
programmers and source code~\cite{Ashok09}.  By comparison, our technique
is more lightweight and focuses only on searching the code and the
defect report.

Jones \textit{et al.} developed Tarantula, a technique that performs
fault localization based on the insight that statements executed often
during failed test cases likely account for potential fault
locations~\cite{harrold05}. Their approach is quite effective when
a rich, indicative test suite is available and can be run as part of
the fault localization process. It thus requires the fault-inducing
input but not any natural language defect report. By contrast,
our approach is lightweight, does not require an indicative test
suite or fault-inducing input, but does require a natural language
defect report. Both approaches will yield comparable performance, and
could even be used in tandem.

Cleve and Zeller localize faults by finding differences between
correct and failing program execution states, limiting the scope of
their search to only variables and values of interest to the fault in
question~\cite{cleve05}. Notably, they focus on those variable and
values that are relevant to the failure and to those program execution
points where transitions occur and those variables become causes of
failure. Their approach is in a strong sense finer-grained than ours:
while nothing prevents our technique from being applied at the level
of methods instead of files, their technique can give very precise
information such as ``the transition to failure happened when $x$
became 2.'' Our approach is lighter-weight and does not require
that the program be run, but it does require defect reports.

Renieris and Rice use a ``nearest neighbor'' technique in their
Whither tool to identify faults based on exposing differences in
faulty and non-faulty runs that take very similar executions
paths~\cite{Renieris03}. They assume a large number of correct runs
(e.g., normal test cases) and one failing run. Their approach uses a
distance criterion to select the correct run that is closest to the
failing run and produces a report of ``suspicious'' parts of the
program. By comparison, we chose to limit the programmatic information
used by our technique to only that which was reported by users: we
do not use test case runs but do need natural language.

Liblit \textit{et al.} use Cooperative Bug Isolation, a statistical
approach to isolate multiple bugs within a program given a deployed user
base. By analyzing large amounts of collected execution data from real
users, they can successfully differentiate between different causes of
faults in failing software~\cite{liblit05}. Their technique produces
a ranked list of very specific fault localizations (e.g., ``the fault
occurs when $i > arrayLen$ on line 57''). In general, their technique
can produce more precise results than ours, but it requires a set of
deployed users and works best on those bugs experienced by many users.
By contrast, we do not require that the program be runnable, much less
deployed, and use only natural language defect report text.

\section{Conclusion}

\bibliographystyle{abbrv}
\bibliography{nanomaly,sw,temp}

\end{document}
