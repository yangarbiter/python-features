% 10+2 Due Fri 28 Aug anywhere-on-earth
%
% ICSE 2019 Technical Track submission must not exceed 10 pages,
% including all text, figures, tables, and appendices; two additional pages
% containing only references are permitted.
%
% ICSE 2019 Technical Track will employ a double-blind review process.
% Thus, no submission may reveal its authorsâ€™ identities.
%
% https://2019.icse-conferences.org/track/icse-2019-Technical-Papers#Call-for-Papers

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption} 
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{myref}
\usepackage{url}
\usepackage{pgfplots}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\fixme}[1]{\textcolor{red}{FIXME: #1}}

\newcommand\lt[1]{{\lstinline|#1|}}
\lstset{language=python}
\definecolor{dkgreen}{rgb}{0,0.5,0}
\definecolor{dkred}{rgb}{0.5,0,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\lstset{basicstyle=\ttfamily\bfseries\footnotesize,
  morekeywords={virtualinvoke},
  keywordstyle=\color{blue},
  ndkeywordstyle=\color{red},
  commentstyle=\color{dkred},
  stringstyle=\color{dkgreen},
  numbers=left,
  numberstyle=\ttfamily\footnotesize\color{gray},
  stepnumber=1,
  numbersep=10pt,
  backgroundcolor=\color{white},
  tabsize=4,
  showspaces=false,
  showstringspaces=false,
  xleftmargin=.23in
}

\pagestyle{plain}
\pagenumbering{arabic}

\newsavebox{\verbsavebox}

\begin{document}

%
\title{Blaming the Typeless: \\ Scalable, Human-Centric Python Localization}

\iffalse

\author{\IEEEauthorblockN{Benjamin Cosman}
\IEEEauthorblockA{\textit{UC San Diego}\\
blcosman@eng.ecsd.edu}
\and
\IEEEauthorblockN{Leon Medvinsky}
\IEEEauthorblockA{\textit{UC San Diego}\\
lmedvinsky@eng.ecsd.edu}
\and
\IEEEauthorblockN{Ranjit Jhala}
\IEEEauthorblockA{\textit{UC San Diego}\\
jhala@cs.ecsd.edu}
\and
\IEEEauthorblockN{Westley Weimer}
\IEEEauthorblockA{\textit{University of Michigan}\\
weimerw@umich.edu}
}

\fi

\author{\IEEEauthorblockN{\emph{submitted for double-blind review}}}

\maketitle

\begin{abstract}
Abstract
\end{abstract}

\section{Introduction}

Dynamically-typed languages are becoming increasingly common for rapid
application development, full-scale software engineering, and
pedagogy (e.g.,~\cite{FIXME,FIXME}). Languages like Python, Ruby and Lua
often feature a gentle learning curve and attractive facilities, from
familiar garbage collection and regular expressions to more expressive and
exotic list comprehensions and higher-order function blocks. One side
effect of popularity with novices and students, however, is that many
programmers of such languages may lack expertise when presented with
dynamic type errors, and may thus struggle to interpret and localize
them~\cite{FIXME,FIXME}. We propose a machine learning approach based on
static, dynamic and contextual information that effectively localizes
beginner type errors.

The relative costs and benefits of static vs. dynamic error detection are
well-studied. Despite this, however, many currently-available techniques
are ill-suited to address the problem of localization for type errors in
dynamic languages. First, relying on the extant language interpreter
requires test inputs~\cite{FIXME}, may involve error messages that are
difficult for novices to understand~\cite{FIXME}, and may misleadingly
implicate the symptom rather than the cause~\cite{FIXME}.  Second, a
significant amount of development is carried out via IDE or web interfaces,
which require tools that operate on code fragments~\cite{Guo2013-vu}.
Third, attempts to retrofit static type systems into dynamic languages,
such as TypeScript for JavaScript~\cite{FIXME} and FIXME for
Python~\cite{FIXME} are promising but not yet as widely
deployed~\cite{FIXME}. Finally, research tools and algorithms for fault
localization either require rich static types (e.g., as in
Mycroft~\cite{FIXME}, Nate~\cite{FIXME} or Sherrloc~\cite{FIXME}), require
test cases (e.g., as in Tarantula~\cite{FIXME}), or produce voluminous
ranked output lists that have been found to be unhelpful in general and
less useful to novices in particular~\cite[Sec.~5.1]{orso-parnin}.

We focus on Python as an indicative, popular dynamically-typed language.
We desire a fault localization algorithm for Python type errors that
will agree with human intuition (\emph{accuracy}), will apply to
off-the-shelf, unannotated beginner-written program fragments
(\emph{generality}), and will scalably apply to industrial-strength
deployments (\emph{scalability}). Our key insights are (1) that we can leverage
modern supervised machine learning for accuracy and an agreement with human
norms; (2) that we can use static, dynamic, contextual and slice-based
features to generalize to off-the-shelf beginner programs; and (3) that we
can use large beginner datasets, such as those from
PythonTutor.com~\cite{Guo2012-vu}, to assess scalability.

We propose an algorithm that takes as input a Python program fragment that
produces an uncaught runtime exception such as a type error. Via machine
learning over a carefully-selected set of program-relevant features, we
produce the expression most likely implicated in the exception according to
our learned model. We evaluate on an entire year of buggy Python 3 program
executions on PythonTutor.com --- over 270,000 instances --- measuring
accuracy with respect to the human actions as the ground truth and
comparing against the vanilla Python interpreter as a baseline.

The contributions of this paper are as follows:
\begin{enumerate}

\item We present an algorithm for accurately localizing dynamic type errors
in Python. We find that FIXME.

\item We describe and evaluate static, dynamic, contextual and slice-based
features for generalizing machine-learning fault localization to
beginner-written Python fragments. We find that FIXME.

\item We analyze the results of a thorough evaluation on over 270,000 real
user instances, demonstrating the scalability of our algorithm.

\end{enumerate}

\section{Motivating Example}
\label{sec-motex}

\begin{figure}
\begin{lstlisting}
year = int(time.strftime("%Y"))
age = input("Enter your age")
print("You will be twice as old in:")
print(year + age)
\end{lstlisting}
\caption{
\label{fig-motex}
A program with multiple fault localizations (lines 1 and 2).}
\end{figure}

One of the key difficulties in localizing type errors, especially for
dynamically-typed programs, is that there are often multiple
logically-valid ways to address a type error~\cite{FIXME} --- only one of
which may be desired by the programmer~\cite{FIXME}.  Consider the program
in \figref{motex}, adapted from our dataset of human submissions. The
program attempts to carry out some simple arithmetic based on a given
number and the current year. When executed, however, the program raises an
exception on line 4, related to the addition of an integer variable to a
string variable.

One reasonable fix (and the one that the programmer actually used in this
case) is to add an \lt{int()} cast on line 2. However, another possible
``fix'' would be to \textit{remove} the \lt{int()} cast from line 1, in
which case both variables hold string values at runtime, and the addition
on line 4 would be interpreted as string concatenation. While the second
fix creates a well-typed program, it is less likely to correspond to
developer intent, and is thus less likely to be helpful as a debugging aid.

This simple example highlights a key fault localization challenge in such
dynamically-typed settings: a runtime type error alone may not contain
enough information to pinpoint the human-preferred localization from among
many valid-but-less-helpful localizations. Our insight is that the relevant
information is not in the types (e.g., string or integer) alone but in the
context in which the values are used.  We propose an approach based on
machine learning over a combination of static and dynamic contextual
features.

% TODO: would also be nice if the example showed that the place Python crashes
% may not be a place that should be fixed?

\section{Algorithm Overview}

We present an algorithm for accurately localizing faults~\cite{tarantula} in
dynamically-typed, beginner-written Python programs that exhibit
non-trivial uncaught runtime exceptions. We do not consider syntax errors
or references to undefined variables. Our algorithm uses machine learning
models based on static, dynamic, contextual and slicing features to
implicate suspicious expressions. Since studies have found that voluminous
fault localization output is not useful to
developers~\cite{orso-parnin,orso-parnin2015}, we focus on producing Top-1
and Top-3 rankings.

Our algorithm first extracts static and dynamic features from a Python
program (Section~\ref{sec-features}). Next, using a labeled training
corpus, we learn a machine learning model over those features
(Section~\ref{sec-model}). Once the model has been learned, we localize
faults in new Python programs by extracting their features and applying the
model.

Drawing inspiration from localization algorithms such as
Nate~\cite{learning-to-blame} and the natural language processing term
frequency vector (or ``bag of words'') model~\cite{FIXME}, we represent
each buggy program as a ``bag of abstracted terms''. A \emph{term} is
either a statement or expression.

\subsection{Model Feature Intuition and Extraction}

We observe that many type errors admit multiple logically valid resolutions
(see \secref{motex}): we thus cannot expect to match programmer desires
through type constraints alone, and instead propose to leverage information
from multiple modalities. We use static features to capture structured
program meaning, contextual features to capture the relationship between a
program fragment and its environment, and dynamic features (including slice
information) to reason about conditional behavior.

For each term in the program we compute static, dynamic, and contextual
features. While some features can be computed directly from syntactic
information, others are only accessible dynamically. In a
dynamically-typed language such as Python, this includes types of each
expression. Since programs are highly structured and expressions gain
meaning in relation to their surroundings, we also include contextual
features.

While static features can be computed via parsing, dynamic features require
executing the program fragment. To calculate dynamic features, we first
convert each buggy program to an equivalent program in a form similar to
A-normal form \cite{anf}: all intermediate expressions are split out and
assigned to new temporary variables.
We then run the resulting program through the PythonTutor
backend~\cite{Guo2013-vu}.  This backend is based on the Python debugging
library BDB~\cite{FIXME}; it returns a trace of the program along with the
state of the heap at each execution step. That trace is used to compute
dynamic features.

\subsection{Model Features}
\label{sec-features}

\subsubsection{Syntactic Forms (Static)}

We hypothesize that certain syntactic categories of terms may be more
prone to bugs than others, especially for beginner programmers.  For
example, students might have more trouble with loop conditions than with
simple assignments. The first feature we consider is the syntactic
category of node. This feature is categorical, using standard abstract
syntax tree categories such as Return or Import for statements and Variable
or Application for expressions.

\subsubsection{Expression Size (Static)}

This numeric feature counts the number of descendent nodes in the subtree
rooted at that node. Our intuition is that larger, more complex expressions
may be more prone to faults.

\subsubsection{Type (Dynamic)}

Although type constraints alone cannot always match human intuition for
fault localization problems, types still contain very useful information.
We observe that some types may be inherently suspect whenever they appear,
especially for beginner-written code. For example, there are very few
reasons to have a variable of type \lt{NoneType} in Python. We model
type information as a categorical feature. Possible
values include all the basic Python types (such as \lt{int}
and \lt{tuple}) as well as three special values:
\begin{itemize}
    \item \lt{Statement} --- Given to all statements, since our terms
    include both statments and expressions but only expressions
    have types
    \item \lt{Unknown} --- Given to expressions that are never evaluated in the dynamic
    trace
    \item \lt{Multiple} --- Given to expressions which are evaluated multiple times in
    a trace and do not always have the same type
\end{itemize}

\subsubsection{Slice (Dynamic)}
The goal of this feature, roughly equivalent in purpose to the type error
slice in Nate~\cite{learning-to-blame}, is to help eliminate terms that
cannot be the source of the crash. We compute a dynamic program
slice~\cite{KOREL1988155}: a set of terms that contributed at runtime to
the observed exception. This boolean feature encodes whether the term
is a member of the slice. Our slicing algorithm, which balances scalable
simplicity with coverage for beginner-written programs, is presented in
\secref{slice-algorithm}. 

\subsubsection{Crash Location (Dynamic)} We observe that the precise term that
causes Python to crash is, in fact, the one the user decides to change
much of the time. While this information is not sufficient on its own
(cf. the vanilla Python interpreter, which largely uses this feature),
it can be a very helpful supplement. This boolean feature encodes whether
or not the term is exactly the source location of the uncaught exception.

\subsubsection{Exception Type (Dynamic)} The type of error thrown by the program
contains useful information for its localization. In particular, a term may be
more or less suspicious depending on the ultimate uncaught exception. As an
example, every division term may be more suspicious if the uncaught exception
being considered is \lt{DivisionByZero}. We encode the exception type as
a categorical feature.
% to \emph{every} vector derived from that program.

\subsubsection{Contextual features}

Term frequency vectors in general, and our bag of abstracted terms
representation by extension, do not typically include contextual
information such as ordering or relative positioning. As with many other
analyses, such as contextual operational semantics~\cite{FIXME}, we observe
that the meaning of an expression may depend on its surrounding context.

As an example, consider the \lt{0} terms in both \lt{x = 0} and \lt{x / 0}.
Both instances of \lt{0} have the same syntactic form, size and type
(etc.). We may prefer to implicate the \lt{0} in \lt{x / 0} as suspicious,
especially for beginner-written programs, but cannot distinguish it without
surrounding contextual information. Data structures such as abstract syntax
trees and control flow graphics capture such contextual information, but
are not immediately applicable to machine learning.

We desire to encode such information while retaining the use of scalable,
accurate off-the-shelf machine learning algorithms that operate on feature
vectors. We thus propose to embed contextual information in a vector,
borrowing insights from standard approaches in machine learning. We
associate with each term additional features that correspond to the features of
its parent and child nodes. For representational regularity, we always
model three children; terms without parents or children are given special
values (e.g., zero or NotApplicable) for those contextual features.

\subsection{Dynamic Slicing Algorithm}
\label{sec-slice-algorithm} 

\begin{figure}
\begin{lrbox}{\verbsavebox} 
\begin{lstlisting}[xrightmargin=0.5\linewidth]
x = input() # 42
if x < 20:
  y = 5
  z = 123
else:
  y = 0
  z = 321
assert(y != 0)
\end{lstlisting}
\end{lrbox} 
~ \hfill
\subcaptionbox{Pre-Slicing}{\usebox{\verbsavebox}}
\hfill
\begin{lrbox}{\verbsavebox}
\begin{lstlisting}[xrightmargin=0.5\linewidth]
x = 42
if x < 20:


else:
  y = 0

assert(y != 0)
\end{lstlisting}
\end{lrbox} 
\subcaptionbox{Post-Slicing}{\usebox{\verbsavebox}}
\hfill
~
\caption{
  A program being sliced (left) and the resulting slice (right) with
  respect to the assertion on line 8. 
}
\label{fig-slice-example}
\end{figure}

\begin{figure}
\begin{lstlisting}
x = input() # The user inputs 0
if x != 0:
  x = 1
print("One over your number is: %d" % (1 / x))
\end{lstlisting}
\caption{In this program, the programmer intended the placeholder
  assignment \lt{x = 1} to be used in case the user inputs 0. 
  The defect is that the 
  incorrect condition \lt{x != 0} (rather than \lt{x == 0}) was employed. However, the
  condition expression will not be included in the slice (which would include
  lines 1 and 4), because the presence of
  the conditional does not affect the behavior of the program.
}
\label{fig-slice-downside-example}
\end{figure}

\begin{figure}
\begin{lstlisting}
while true:
  x = input()
  if x != 0:
    break
  print("One over your number is: %d" % (1 / x))
\end{lstlisting}
\caption{
  In this example, the programmer attempts to handle invalid input by
  breaking out of the input loop. Our method
  records a control dependency between the condition on line 3 and the
  print statement on line 5, correctly placing the buggy condition in the
  program slice.
}
\label{fig-early-break}
\end{figure}

Slicing information can prevent our model from implicating irrelevant
nodes in the fault localization. Program slicing is a well-studied
field with many explored tradeoffs (e.g., see Xu et al. for a
survey~\cite{xu2005}). We desire a slicing algorithm that can be computed
efficiently (to scale to hundreds of thousands of instances) but that will
also admit high accuracy: we achieve this by focusing on features relevant
to beginner-written programs. We
follow the basic approach of Korel and Laski~\cite{KOREL1988155,
KOREL1990187}, building a
graph of data and control dependencies. We then traverse the graph
backwards, starting at the execution step where the error occurred, to
collect the set of terms that the excepting line transitively depended on.
This excludes lines that could not have caused the exception, such as lines
that never ran, or lines that ran but had no connection to the step where
the exception happened.

\figref{slice-example} is an example of the input and output of the slicing
algorithm. The slicer takes as input a python source string and a list of inputs,
to be fed into the program whenever \lt{input()} is invoked. The slicer outputs
the program elements included in the slice.

The slicer proceeds by creating an execution trace of the example program. 
At each execution step the trace includes the line number being run and the state of the
heap and variables. In the example in \figref{slice-example}, 
lines $1, 2, 3, 4$ and $8$ are executed. 
It then iterates forward through the trace, recording which
execution steps have a define-use relation, and which execution steps
have a test-control relation. The define-use relation is between a step where
the value of a variable is used, and the step where that variable was last
defined. In \figref{slice-example}, a define-use relationship exists
between lines 1 and 2 and between 3 and 8. A test-control relation is between
the execution of a control flow statement and any statements that were run as
a result of it. In \figref{slice-example}, a test-control relationship exists
between the if statement at line 2 and the assignments on lines 3 and 4. After
building up these relations, the slicer starts at the execution step causing an
exception (the execution of line 8), and builds the exception's set of
dependencies by iteratively adding to the set any step that has a define-use
or test-control relationship with a step already in the set. Finally, the
algorithm maps all of the execution steps to their corresponding program 
element locations and returns the result. 

To balance ease of prototype implementation against coverage for
beginner-written programs, our slicing algorithm handles every syntax node
supported by the Python3 standard \lt{ast} library except:
\begin{itemize}
\item Assignments where the left hand side is not a variable or a simple chain
  of attribute indexing or subscripting
\item Assignments where attribute indexing or subscripting on the left hand side
  means something other than the default, (e.g., if the operations are
  overridden by a class)
\item Lambda, generator and starred expressions
\item Set and dictionary comprehensions
\item Await, yield, and yield from
\item Variable argument ellipsis 
\item Coroutine definitions and asynchronous loops
\item Delete, with and raise statements
\end{itemize}

A downside of using a dynamic (as opposed to static) slice, in the case of
control dependencies, is that it excludes terms whose presence does not
affect whether the exception happened, but which could have had an effect
if the term was different. In
\figref{slice-downside-example}, the conditional statement is never
executed, so the removal of the conditional would not have any effect on
the program's behavior, and it would not be included in a more precise
program slice. In this example, the dynamic slice misses the incorrect
test condition because the very defect we are trying to localize
caused the conditional statement and its body to become irrelevant.

To overcome these issues in the ``early return'' or ``early break'' case
(\figref{early-break}), we check if a break, return, or other statement for
escaping structured control flow is present inside of a conditional
statement. We then add dependencies in the execution trace's dependency graph
between the enclosing conditional and the statements that would have been skipped
by the break or return. While this heuristic is effective in practice, it
does not overcome all related problems: we thus treat dynamic slice
information as one of many features rather than as a hard constraint.
This boolean feature tracks whether or not the term is part of the final
dynamic slice.

\subsection{Machine Learning Model Generation}
\label{sec-model}

\fixme{
\emph{Models} The following descriptions of Logistic Regression, Decision Trees,
Random Forests, and MLP will have to be redone as they are copied from Learning to Blame.
}

\emph{Logistic Regression}
The simplest classifier we investigate is logistic regression:
a linear model where the goal is to learn a set of weights $W$
that describe the following model for predicting a label
$b$ from a feature vector $v$:
%
\[ \Pr(b = 1 | v) = \frac{1}{1 + e^{-W^{\top} v}} \]
%
The weights $W$ are learnt from training data, and the value of
$\Pr(b | v)$ naturally leads to a confidence score $\mathcal{C}$.
%
Logistic regression is a widely used classification algorithm, preferred
for its simplicity, ease of generalization, and interpretability.
%
Its main limitation is that the prediction rule is constrained to be a
linear combination of the features, and hence relatively simple.
%
While this can be somewhat mitigated by adding higher order (quadratic
or cubic) features, this often requires substantial domain knowledge.

\emph{Decision Tree}
Decision tree algorithms learn a tree of binary predicates over the
features, recursively partitioning the input space until a final
classification can be made.
%
Each node in the tree contains a single predicate of the form
$v_j \leq t$ for some feature $v_j$ and threshold $t$, which determines
whether a given input should proceed down the left or right subtree.
%
Each leaf is labeled with a prediction and the fraction of
correctly-labeled training samples that would reach it; the latter
quantity can be interpreted as the decision tree's confidence in its
prediction.
%
This leads to a prediction rule that can be quite expressive depending
on the data used to build it.

Training a decision tree entails finding both a set of good partitioning
predicates and a good ordering of the predicates based on data.
%
This is usually done in a top-down greedy manner, and there are several
standard training algorithms such as C4.5 \cite{Quinlan1993-de} and
CART \cite{Breiman1984-qy}.

Another advantage of decision trees is their ease of interpretation ---
the decision rule is a white-box model that can be readily described to
a human, especially when the tree is small.
%
However, the main limitation is that these trees often do not generalize
well, though this can be somewhat mitigated by \emph{pruning} the tree.

\emph{Random Forests}
%
Random forests (RF)~\cite{breiman2001random} improve generalization by training an
\emph{ensemble} of distinct decision trees and using a majority
vote to make a prediction.
For each tree, RF selects a random sample with replacement of the training
set and trains the tree with these samples.
%
The agreement among the trees forms a natural
confidence score.
%
Since each classifier in the ensemble is a decision tree, this still
allows for complex and expressive classifiers.

The training process involves taking $N$ random subsets of the training
data and training a separate decision tree on each subset --- the
training process for the decision trees is often modified slightly to
reduce correlation between trees, by forcing each tree to pick features
from a random subset of all features at each node.
%
The diversity of the underlying models tends to make random forests less
susceptible to the overfitting, but it
also makes the learned model more difficult to interpret.
\emph{Random Forest}


\emph{Neural Networks}
%
The last (and most complex) model we use is a type of neural network
called a \emph{multi-layer perceptron} (see \cite{Nielsen2015-pu} for
an introduction to neural networks).
%
A multi-layer perceptron can be represented as a directed acyclic
graph whose nodes are arranged in layers that are fully connected by
weighted edges.
%
The first layer corresponds to the input features, and the final to the
output.
%
%The output of a node $v$ in an internal layer is given by:
The output of an internal node $v$ is
%
\[ h_v = g(\sum_{j \in N(v)} W_{jv} h_j ) \]
%
where $N(v)$ is the set of nodes in the previous layer that are adjacent
to $v$, $W_{jv}$ is the weight of the $(j, v)$ edge and $h_j$ is the
output of node $j$ in the previous layer.
%
Finally $g$ is a non-linear function, called the activation function,
which in recent work is commonly chosen to be the \emph{rectified linear
  unit} (ReLU), defined as $g(x) = \mathsf{max}(0,x)$
\cite{Nair2010-xg}.
%
The number of layers, the number of neurons per layer, and the
connections between layers constitute the \emph{architecture} of a
neural network.
%
In this work, we use relatively simple neural networks which have an
input layer, a single hidden layer and an output layer.

A major advantage of neural networks is their ability to discover
interesting combinations of features through non-linearity, which
significantly reduces the need for manual feature engineering, and
allows high expressivity.
%
On the other hand, this makes the networks particularly difficult to
interpret and also difficult to generalize unless vast amounts of
training data are available.

\emph{Training methodology} At this point we have feature vectors describing every term of every program in
our dataset. The vectors from a random 30\% of the program pairs are set aside for testing and
the other 70\% are used for training.
Within the training data, we use 3-fold
cross-validation to select the best parameters for our models, before training
using those parameters on the full training set and then scoring the models
using the testing set.
Each trained model takes in any vector representing a term
in a buggy program, and returns a confidence score representing how likely it is that
that term was one of the terms changed between the fixed and buggy versions.
Thus we can treat the model as providing a \emph{ranking} over all terms by
confidence, where the top result is the one that the model deems most likely
to be changed.
For a given k, we score the model based on Top-k accuracy: for what
proportion of the programs is a correct answer (i.e. a term that is actually
changed) present in the top k results.
We retrain the models for each value of k, since a model optimized to produce
the single best result might be different from one optimized to get a good
result into the top three.

For decision tree and random forest, we use the implementation from
\textsc{scikit-learn}~\cite{scikit-learn}.
Due to the large amount of data points, we did not use \textsc{scikit-learn}'s
implementation for logistic regression and multilayer perceptron.
We implementation both model using
\textsc{keras}~\cite{chollet2015keras} with mini-batch gradient descent.

Because of the imbalance nature of the data set, where not-blamed lines
(negative examples) appears more frequently than blamed lines (positive
examples).
The weight for each example during training is re-weighted to the reciprocal
of the frequency of corresponding class.

\paragraph{Hyper-parameters}
All hyper-parameters are tuned using three-fold cross-validation on the
intended score (Top-k accuracy).
For RF, the maximum depth of each tree is searched from $25$ to $35$.
For decision tree maximum depth of the tree is searched from $25$ to $35$ with interval of $5$,
the minimum impurity decrease for each split is searched from $10^{-7}$ to
$10^{-3}$ with multiple of $10^{-2}$ and the minimum samples in each leaf is set to $200$.
For logistic regression and MLP, binary crossentropy is set as the loss
function and Adam \cite{kingma2014adam} optimizer is used.
Learning rate is set to $0.1$.
L2 regularization is applied to the weights of both models,
the weight for L2 term is searched from $10^0$ to $10^{-5}$ with a multiple of $10^{-1}$.
Other parameters that are not mentioned are kept with the default setting of the
original packages.

In each case the most effective model we trained was a decision tree. (The other
models we tried were Logistic Regression and MLP. Yao-Yuan - anything to add to
this section? in particular, is it actually accurate to say we're doing
cross-validation - the 3-fold thing we do is actually just to select parameters,
and then there's no cross-validation on the final model, right? 
%
Correct
)


\section{Evaluation}
\label{sec-eval}

We conducted a large-scale empirical evaluation of our algorithm with the
aim of addressing a number of research questions:
\begin{enumerate}

\item[RQ1]{Can we accurately localize non-trivial faults in Python
programs?}

\item[RQ2]{Which features are the most important for Python fault
localization overall?}

\item[RQ3]{Which features are the most important for various categories of
Python defects?}

\item[RQ4]{Is our algorithm accurate on heterogeneous sets of programs?}

\end{enumerate}

\subsection{Dataset and Program Collection}

Our raw data consist of every Python 3 program that a user executed on
PythonTutor.com~\cite{Guo2013-vu} (not in ``live'' mode) during 2017, other
than those with syntax errors or undefined variables.  Each program which
crashes (throws an uncaught Python exception) is paired with the next
program (by the same user) that does not crash, under the assumption that
the latter is the fixed version of the former. We discard pairs where the
difference between crashing and fixed versions is too high (more than a
stddev above average), since these are most likely to be violations of that
assumption (i.e., the program that does not crash is unrelated to the
crashing program). We also discard submissions that are out of scope given
PythonTutor's policies (e.g., very long-running executions or the
use of forbidden libraries).

Ultimately, the dataset used in this evaluation contained just over
270,000 usable pairs of programs.

\subsection{Labeled Training and Ground Truth}

Our algorithm is based on supervised machine learning and thus requires
labeled training instances. In addition, our evaluations require a ground
truth notion of which terms correspond to correct fault localizations.

We use the terms changed involved in the fixes by actual users as our
ground truth and training information.  Many PythonTutor users use the site
in an iterative fashion: they start out by writing a program that crashes,
and then edit it until it no longer crashes. Our dataset contains only
those crashing programs for which the same user later submitted a program
that did not crash. We compute a tree-diff \cite{tree-diff} between the
originaly, buggy submission and the first fixed submission. We define
the ground truth correct answer to be the set of terms in the crashing
program that also appear in the diff. We discuss the implications of this
choice in \secref{threats}.

In our evaluation, we used FIXME as a training set and FIXME as a held-out
evaluation set. We employed cross-validation~\cite{kohavi} to help address
the potential threat of overfitting.

\subsection{Methodology}

For these evaluations, we employ decision trees as our machine learning
algorithm, since they provided the best accuracy on our data given the
available training time.

We report Top-1 and Top-3...

We define FIXME-ACCURACY to be FIXME.

\subsection{RQ 1 --- Fault Localization Accuracy}

\begin{figure}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    symbolic x coords={Baseline,Top-1,Top-2,Top-3},
    xtick=data,
    ymin=0,
    ylabel=Model Accuracy,
    enlarge x limits=0.3,
    legend style={at={(0.5,-0.15)},anchor=north}
]
    \addplot table[x=scoreName, y=score, col sep=comma]{fault-localization.csv};
\end{axis}
\end{tikzpicture}
\caption{Fault localization accuracy. 
Baseline is the normal Python interpreter. Other bars represent our
approach, using decision trees, 
on the largest dataset (272534 pairs).}
\label{fig-full-dataset-acc}
\end{figure}

\begin{figure}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    symbolic x coords={Baseline,Top-1,Top-3},
    xtick=data,
    ymin=0,
    ylabel=Model Accuracy,
    enlarge x limits=0.3,
    legend style={at={(0.5,-0.15)},anchor=north}
]
    \addplot table[x=scoreName, y=score, col sep=comma]{fault-localization-random-forest.csv};
\end{axis}
\end{tikzpicture}
\caption{Fault localization accuracy. 
Baseline is the normal Python interpreter. Other bars represent
our approach, using random forests, 
on the largest dataset (272534 pairs). \fixme{get top-2, then change text
to discuss this figure instead of decision-tree version}}
\label{fig-full-dataset-acc-random-forest}
\end{figure}

We first use the full dataset to train and test decsion trees optimized for
Top-1, Top-2, and Top-3 accuracy. As shown in \figref{full-dataset-acc},
these produce a correct answer 55, 68, and
75\% of the time (to two significant figures). As a baseline, we determine that
the expression blamed by the standard Python interpreter (i.e. the one that
causes it to crash) is only changed by the user 30\% of the time. Thus our
most directly comparable model, the one for Top-1, significantly outperforms
this baseline, and as we then exercise our power to give a ranked output rather
than a single answer, our margin gets even better.

% Goal: Establish that our algorithm works. This is the most basic question,
% but also the most relevant. If the reader takes away nothing else, they
% should conclude that our algorithm is effective.

\subsection{RQ 2 --- Feature Predictive Power}

\begin{table}[]
\begin{tabular}{llc}
Name & Category & Gini Importance \\ \bottomrule
Size-P & Contextual (Syntactic)               & 0.112 \\
Stmt-Constr-C3= & Contextual (Syntactic)      & 0.061 \\
Expr-Constr-P=List & Contextual (Syntactic)   & 0.055 \\
PythonBlame & Dynamic (Error location)        & 0.039 \\
Type=unknown & Dynamic (Type)                 & 0.037 \\
Type-P=unknown & Contextual (Type)            & 0.037 \\
PythonMsg=IndexError & Dynamic (Error message)& 0.033 \\
Size & Syntactic                              & 0.028 \\
Expr-Constr-P=Dictionary & Contextual (Syntactic) & 0.027 \\
Size-C1 & Contextual (Syntactic)              & 0.024 \\
Expr-Constr=Var & Syntactic                   & 0.020 \\
Type-C1=unknown & Contextual (Type)           & 0.020 \\
\toprule
\end{tabular}
\caption{Feature predictive power (for a Top-3 Decision Tree
learned on the entire dataset).}
\label{tab-feature-predictive-power}
\end{table}

\begin{table}[]
\begin{center}
\begin{tabular}{llc}
Name & Category & F-score \\ \bottomrule
Stmt-Constr-C3= & Contextual (Syntactic) & 69 \\
Size & Syntactic                         & 63 \\
PythonBlame & Dynamic (Error location)   & 50 \\
Type-P=LIST & Contextual (Type)          & 36 \\
Type-C1=bool & Contextual (Type)         & 30 \\
Type-P=unknown & Contextual (Type)       & 28 \\
Type-C1= & Contextual (Type)             & 25 \\
Size-C3 & Contextual (Syntactic)         & 20 \\
Op-Constr-C1= & Contextual (Syntactic)   & 20 \\
Size-C1 & Contextual (Syntactic)         & 18 \\
Type-P=DICT & Contextual (Type)          & 17 \\
Type=str & Dynamic (Type)                & 12 \\
\toprule
\end{tabular}
\end{center}
\caption{Feature predictive power as measured by ANOVA.  The F-score in
shown in 
thousands to two significant figures; all p-values are less than 0.01.}
\label{tab-anova}
\end{table}

\begin{figure}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    symbolic x coords={noContext, noTypes, all},
    xtick=data,
    ymin=0,
    ylabel=Model Accuracy,
    enlarge x limits=0.5,
    legend style={at={(0.5,-0.15)},anchor=north}
]
    \addplot table[x=scoreName, y=score, col sep=comma]{removing-features-2.csv};
\end{axis}
\end{tikzpicture}
\caption{Accuracy when features are removed, based on decision trees on a
random subset of 20000 program pairs.
\fixme{The x-axis labels are not perfectly clear: does ``all'' mean that
everything is still in or that everything is out?}
\fixme{Indicate that this is a top-1 model.}
\fixme{Normal the y-axis so that ``all'' is 1.0 to make the subset effects
more evident.} 
}
\label{fig-removing-features}
\end{figure}

\tabref{feature-predictive-power} summarizes the relative importance
of the top features in our model. The features are ranked by their
Gini importance (or mean decrease in impurity), a common measure
for decision tree and random forest models~\cite{FIXME}. Informally, the
Gini importance conveys a weighted count of the number of times a feature
is used to split a node: a feature that is learned to guide more model
classification decisions is more important. Similarly, \tabref{anova} gives
an alternate view of the relative feature importances, as measured by an
analysis of variance (ANOVA). 

We also present the results of a leave-one-out analysis in which 
certain categories of features are removed and the model is trained
and tested only on those that remain. \figref{removing-features} shows
that there is a significant decrease in accuracy in each case. We note,
however, that leave-one-out analyses underestimate importance in cases of
feature overlap. For example, if a small program contains a string bug but
only a few string variables, both type information and slice information
are likely to implicate similar terms. 

In all cases we see that syntactic, dynamic, and contextual features are
all important. Features such as FIXME, FIXME and FIXME are found to be of
primary importance. We interpret this to mean FIXME with respect to the
application of our algorithm to beginner-written code. 

% prereq: define predictive power (leave-one-out, leave-one-in, Relief-F,
% ANOVA, whatever)

% Use as a rough guide: Table 3 on Page 8 of
% https://web.eecs.umich.edu/~weimerw/p/weimer-icsm2010.pdf

% Goal: Establish that we are smart for including all of these features. Note
% which individual features or feature categories were (not) included in
% previous work.

% Give a simple narrative about our effectiveness: implicitly, we were smart
% for deciding to include these features, better ingredients make better
% results, we have great results.

% TODO: what about turning on/off slicing?

% TODO: what about turning on/off types? (is that separate from slicing?)

\subsection{RQ 3 --- Defect Categories}

\begin{figure}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    symbolic x coords={All, TypeError, AttributeError},
    xtick=data,
    ymin=0,
    ylabel=Model Accuracy,
    enlarge x limits=0.5,
    legend style={at={(0.5,-0.15)},anchor=north}
]
    \addplot table[x=scoreName, y=score, col sep=comma]{defect-categories-2.csv};
\end{axis}
\end{tikzpicture}
\caption{Accuracy when the model is trained and tested on only the data exhibiting
the error on the x-axis.
\fixme{Spin this for generality. Indicate that our model gives high-quality
fault localization regardless of defect type. To put it another way, we
can view this as a sensitivity analysis: our algorithm is not sensitive to
defect type (that's good!).} 
}
\label{fig:defect-categories}
\end{figure}

prereq: define defect category. This could be either the raw python
exception name or it could be clusters that we have manually created.
Ranjit notes: result could be ``we need everything for everything'', at
which point this is useless and should be skipped. Per Ben's exam: may find
that for different categories of errors we should use different ML
classifiers.

PLOT Type: Bar Graph (three bars per point on the X axis)

PLOT Y Axis: Leave-One-In Accuracy (normalized per X axis point)
Ranjit notes: on smaller subsets we could rebuild the classifier each time.
Wes notes that if we do, it takes us longer. If we re-use the monolithic
model we can do this very quickly.

PLOT X AXis: Defect Categories

PLOT Bars: Static, Dynamic, Context

Goal: Demonstrate that each category of feature is essential for a certain
piece of the problem. We need ABC feature to handle DEF class of defects,
but ABC feature does not work well on GHI class of defects, for those we
need JKL features. Implicit: we were smart for including all of these
categories.

\subsection{RQ 4 --- Diversity of Programs}

% PLOT LINES: line 1 is ``Nate programs'', line 2 is ``25\%, at random, of our dataset''

% \begin{figure}
% \foreach \method in {single, complete, average}
% {
% \begin{tikzpicture}
%   \begin{axis}[
%       xlabel=Threshold,
%       ylabel=\# of clusters,
%     %   ytick distance=2,
%       title=Linkage: \method
%     ]
%     \addplot+[mark=none] table[x=threshold, y=python_twenty_cluster_counts_\method, col sep=comma]{cluster_counts.csv};
% \end{axis}
% \end{tikzpicture}
% }
% \caption{The number of clusters when clustering a baseline of twenty dissimilar programs from
%   Rosetta Code. We give no plot for the baseline of 5000 identical programs, as they were
%   consistently placed in just 1 cluster.}
% \label{fig:diversity-baseline}
% \end{figure}

% \begin{figure}
% \foreach \method in {single, complete, average}
% {
% \begin{tikzpicture}
%   \begin{axis}[
%       xlabel=Threshold,
%       ylabel=\# of clusters,
%     %   ytick distance=1000,
%       title=Linkage: \method
%     ]
%     \foreach \language in {ocaml, python}
%     {
%       \addplot+[mark=none] table[x=threshold, y=\language_cluster_counts_\method, col sep=comma]{cluster_counts.csv};
%     }
% \legend{OCaml, Python}
% \end{axis}
% \end{tikzpicture}
% }
% \caption{Number of clusters when data is clustered by single, complete, and average linkage.
%   A linkage tree is built using agglomerative clustering, and the resulting tree is broken up
%   into clusters by thresholding the inconsistency coefficient. The inconsistency coefficient
%   of a link is its distance, z-scored against other nearby links.}
% \label{fig:diversity}
% \end{figure}

\begin{figure}
\begin{tikzpicture}
  \begin{axis}[
      xlabel=Clustering threshold parameter,
      ylabel=\# of naturally occuring program clusters,
      xmax=1.25,
      legend style={at={(0.5,0.25)},anchor=north}
    %   ytick distance=1000,
    %   title=Linkage: single
    ]
    \foreach \language in {ocaml, python}
    {
      \addplot+[mark=none] table[x=threshold, y=\language_cluster_counts_single, col sep=comma]{cluster_counts.csv};
    }
\legend{NATE OCaml data, Our Python data}
\end{axis}
\end{tikzpicture}
\caption{Number of clusters when data is clustered by single linkage.
  A linkage tree is built using agglomerative clustering, and the resulting tree is broken up
  into clusters by thresholding the inconsistency coefficient. The inconsistency coefficient
  of a link is its distance, z-scored against other nearby links. TODO: find a way to include
  the Rosetta Code baseline in this graph? better not to devote multiple graphs to this issue.
  LM: the y axis could be made into a percentage of the dataset size, but that might be
  weird?}
\label{fig-diversity}
\end{figure}

In the most similar prior work, Learning to Blame, each program in the dataset
was a student's attempt at one of 23 different homework problems. Thus the
dataset was comparatively homogenous, making it unclear whether the results
would generalize to a more diverse dataset.

In our PythonTutor data by comparision, users wrote whatever programs they
wanted, so we
hypothesized that the data were more heterogeneous. To confirm this
quantitatively, we used agglomerative clustering both on our
data and on the Learning to Blame data; whichever dataset naturally splits
into more clusters is more heterogeneous.

% The exact number of clusters returned
% depends on the value we choose for the algorithm's threshold parameter, so we
% checked all thresholds at .01 intervals, starting at 0 and ending at the point
% where everything collapses into a single cluster. As shown in
% \figref{diversity}, regardless of the choice of threshold, our data breaks into
% more clusters than the NATE data. This confirms that our data is more
% heterogeneous.

To cluster programs, we transform the program ASTs into flat strings, and then
perform agglomerative clustering on them, with string edit distance as a distance
metric.

\subsubsection{Transformation and Distance Metric}

To measure the distance between programs, we flattened their ASTs into strings
of tokens, and then computed the Levenshtein edit distance~\cite{levenshtein}.
The decision to flatten the tree rather than compute a distance directly on the
the ASTs is practical; the state of the art in tree edit distance has cubic
asymptotic complexity~\cite{PAWLIK2016157}, while the levenshtein distance can be
computed in time quadratic in the length of the strings~\cite{lev-quadratic}.
When we attempted a test run using tree edit distance as the metric, it did not
complete and we determined that it would not be practical to use.

The flattening process, which is based on \cite{dist-site}, is defined
recursively: the string for an AST
begins with a unique token corresponding to the AST node type, which is
followed by the concatenation of the transformations of the tree's subtrees.
The string ends with an ``end'' token to denote the end of the tree. This
preserves the tree structure of the AST.

\subsubsection{Clustering Algorithm}

We performed agglomerative clustering on the programs~\cite{modern-clustering}.
This is a method of clustering in which every datapoint starts in its own
cluster, and the two closest clusters are merged every step of the algorithm,
until every datapoint is in the same cluster. This produces a binary tree, where
each internal node corresponds to a cluster formed by merging two other clusters,
and each leaf corresponds to an item in the dataset. We used single linkage,
which means that the distance between clusters of multiple items was considered
to be the minimum distance between their elements.

This alone produces a tree rather than a list of disjoint clusters, which makes
different clusterings difficult to compare. To produce a cluster count, we used
the SciPy function \lt{scipy.cluster.hierarchy.fcluster}, which forms disjoint
clusters by splitting the tree at internal nodes where the two joined clusters
are more distant than some threshold. We used the \lt{'inconsistent'} method with
depth set to 2, which specifies that the distances at each node should be
z-scored against the distances at other nodes at the same level in the tree, and
any nodes 1 or 2 levels below~\cite{inconsistent}. This adapts to differences in
scale of edit distance that might arise between python and OCaml due to
programming language expressivity or other factors, making cluster counts at each
threshold value comparable.

\figref{diversity} shows the results of clustering on our dataset and the NATE
dataset. For all values of the inconsistency threshold, the algorithm computed a
higher cluster count for the Python data, suggesting that it contains a more
diverse set of programs than the OCaml dataset. The similarity in the graphs
appears to confirm the comparibility of cluster counts between the two datasets.

\subsection{RQ 5 --- Wishlist: Partitioning Kinds of Errors}

Can we partition the kinds of errors that people make? What are common
misconceptions? Do these correspond to paths of decision trees.

If we don't get to the source-sink thing, it could go here.

I don't think we have time, in this paper, to actually implement a
two-level machine learning.

\subsection{Qualitative Analysis}

Pick out one example where we did poorly. Walk through why it is ``out of
scope'', in some sense: we would need features that capture XYZ to handle
it correctly, but those are either expensive or undecideable. Hint at
future work.

Pick out a few examples where we did well and the baseline did poorly. Walk
through how interesting and indicative they are and explain why we did well.

\subsection{Evaluation Summary}

Summarize the evaluation. One sentence per subsection.

\subsection{Threats to validity}
\label{sec-threats}

\emph{Overfitting and other machine learning issues} Yao-Yuan, how do we avoid
these?

\emph{Language choice} We only showed that this technique works for Python 3.
However, \cite{learning-to-blame} uses a similar technique to great effect in
OCaml, and if it works on these two quite-different languages then we believe
it would work on others as well. Additionally, Python is a common choice of
language for introductory programming courses, so even a technique that worked
only on Python would still be very useful for helping novices.

\emph{Target population} We don't know exactly who our study population is -
who wrote the programs in the dataset and what exactly they were trying to write.
It is likely that they are primarily students and not professional software engineers,
and thus possible that our technique does not generalize well to the kinds of
bugs that would be found in an industrial setting rather than an educational one.

\emph{Automatic labeling} Since our dataset is so large, we were forced to use an automatic
scheme to label bugs rather than marking them by hand. We chose to use the diffs
between each program that crashes and the next program that does not, but this method
is not 100\% accurate because the next program may contain additional changes
beyond what was needed to fix the bug, or it may indeed be an entirely unrelated
program. We mitigate this problem by discarding as outliers program pairs that have
changed too much. We also note that if our `true' accuracy is indeed a little
smaller than our reported accuracy because of this effect, then the `true' baseline
is also lower than the reported baseline for the same reason, so our
\emph{margin} of effectiveness is unlikely to change much.

\section{Related Work}
\fixme{This must all be rewritten or dropped, as it is directly copied from
my previous papers. It serves as a placeholder to indicate length and to
remind us of some Software Engineering bits we might cite.}

There are several fault localization techniques that rely much more heavily
on dynamic information\footnote{Note: we do not directly compare to these
state-of-the-art fault localization techniques for several reasons.  First,
the benchmarks used in this paper do not always have available the test
suites needed by dynamic techniques and, conversely, those used in the
previous work do not have the publicly available bug reports required
by our technique.  Additionally, we return an answer only when confident in
our result set and also employ a top list of all available results, while
previous work is typically evaluated using a score metric.  We consider the
techniques complimentary.}~\cite
{harrold05,Renieris03,cleve05,wang09}.
In general,
these techniques leverage differences between passing and failing program
executions.  While effective, this type of approach requires program traces
for not only the specific fault at hand but also from a comprehensive regression
test suite for comparison.
A hybrid approach that considers both dynamic
traces and static natural language similarity was recently proposed by
Medini \textit{et al.}~\cite{Medini11}.
In contrast to these approaches, our technique relies purely on static
information that is readily available in most commercial systems with minimal
additional developer effort.

Prabhakararao and Ruthruff \emph{et al.} performed two human studies
to gauge the effectiveness of an interactive fault localization tool
developed for end users with little to no
experience~\cite{Prabhakararao03,ruthruff05}.  The goal of their
studies was to evaluate the use of feedback when locating faults and
to generally study the process of fault localization, especially by
users with no expert domain knowledge of the source.  By comparison,
our human study also examines the fault localization process but for
the purpose of evaluating software quality metrics.  We are less
interested in the specific process and more concerned with the
resulting accuracy and the human intuitions about the code in
question.  Additionally, our human study is of a much broader scope and
thus we hope it is more generalizable.
%Additionally, their studies included 10 and 20
%participants respectively using a spreadsheet-based visualization tool.
%Our study of 61 humans looking at actual source code potentially provides
%more generalizable results and targets specifically the area of software
%maintenance completed by trained programmers.


Ashok \textit{et al.} propose a similar natural language search technique
in which users can match an incoming report to previous reports,
programmers and source code~\cite{Ashok09}.  By comparison, our technique
is more lightweight and focuses only on searching the code and the
defect report.

Jones \textit{et al.} developed Tarantula, a technique that performs
fault localization based on the insight that statements executed often
during failed test cases likely account for potential fault
locations~\cite{harrold05}. Their approach is quite effective when
a rich, indicative test suite is available and can be run as part of
the fault localization process. It thus requires the fault-inducing
input but not any natural language defect report. By contrast,
our approach is lightweight, does not require an indicative test
suite or fault-inducing input, but does require a natural language
defect report. Both approaches will yield comparable performance, and
could even be used in tandem.

Cleve and Zeller localize faults by finding differences between
correct and failing program execution states, limiting the scope of
their search to only variables and values of interest to the fault in
question~\cite{cleve05}. Notably, they focus on those variable and
values that are relevant to the failure and to those program execution
points where transitions occur and those variables become causes of
failure. Their approach is in a strong sense finer-grained than ours:
while nothing prevents our technique from being applied at the level
of methods instead of files, their technique can give very precise
information such as ``the transition to failure happened when $x$
became 2.'' Our approach is lighter-weight and does not require
that the program be run, but it does require defect reports.

Renieris and Rice use a ``nearest neighbor'' technique in their
Whither tool to identify faults based on exposing differences in
faulty and non-faulty runs that take very similar executions
paths~\cite{Renieris03}. They assume a large number of correct runs
(e.g., normal test cases) and one failing run. Their approach uses a
distance criterion to select the correct run that is closest to the
failing run and produces a report of ``suspicious'' parts of the
program. By comparison, we chose to limit the programmatic information
used by our technique to only that which was reported by users: we
do not use test case runs but do need natural language.

Liblit \textit{et al.} use Cooperative Bug Isolation, a statistical
approach to isolate multiple bugs within a program given a deployed user
base. By analyzing large amounts of collected execution data from real
users, they can successfully differentiate between different causes of
faults in failing software~\cite{liblit05}. Their technique produces
a ranked list of very specific fault localizations (e.g., ``the fault
occurs when $i > arrayLen$ on line 57''). In general, their technique
can produce more precise results than ours, but it requires a set of
deployed users and works best on those bugs experienced by many users.
By contrast, we do not require that the program be runnable, much less
deployed, and use only natural language defect report text.

\section{Conclusion}

\bibliographystyle{abbrv}
\bibliography{nanomaly,sw,temp,slice,cluster,ml}

\end{document}
